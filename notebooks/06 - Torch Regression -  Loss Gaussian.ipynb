{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a regression with two outputs and Custom Loss Function\n",
    "\n",
    "Use pytorch to take into account the correlation during the training\n",
    "\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variables = 4 # Number of data and background varialbes to generate\n",
    "n_samples = 10000 # Number of samples to generate\n",
    "\n",
    "center_signal = 2.0 # Where the gaussian for signal should be\n",
    "center_background = 1.0 # Where the gaussian for background should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += ['../abcdlib']\n",
    "import data_gen\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = data_gen.generate_gaussian(n_variables, center_signal, n_samples)\n",
    "back = data_gen.generate_gaussian(n_variables, center_background, n_samples)\n",
    "\n",
    "half = int(n_samples/2)\n",
    "\n",
    "training = data_gen.combine(sig[:half], back[:half])\n",
    "testing = data_gen.combine(sig[half:], back[half:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with two outputs and a custom loss function\n",
    "\n",
    "For a regression, we need just a single output column with the 1 or 0 as the target value. Of course, this time we need 2 columns, duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = training[training.columns[-1]].values\n",
    "labels = np.stack((label, label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(training[training.columns[:-1]].values)\n",
    "y_train = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function should be a combination of the normal accuracy loss function and also the correlation on background (only):\n",
    "\n",
    "- Use the accuracy on all the training data\n",
    "- Calculate the correlation only on items marked as being background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r(prediction):\n",
    "    mean = torch.mean(prediction, dim=0)\n",
    "    std_dev = torch.std(prediction, dim=0)\n",
    "    parts = (prediction - mean)\n",
    "    sum = torch.sum(parts[:,0]*parts[:,1])\n",
    "    return sum / std_dev[0] / std_dev[1] / (prediction.shape[0]-1)\n",
    "\n",
    "def calc_average_above(prediction, limit=1.0):\n",
    "    p_mask = prediction > limit\n",
    "    return torch.sum(torch.square(prediction[p_mask]-limit))/prediction.shape[0]\n",
    "\n",
    "def calc_average_below(prediction, limit=0.0):\n",
    "    p_mask = prediction < limit\n",
    "    return torch.sum(torch.square(prediction[p_mask]+limit))/prediction.shape[0]\n",
    "\n",
    "def calc_moment(prediction, n):\n",
    "    'Return the moments for each col of the distribution'\n",
    "    means = torch.mean(prediction, dim=0)\n",
    "    diffs = (prediction - means)\n",
    "\n",
    "    var = torch.mean(torch.pow(diffs, 2.0), dim=0)\n",
    "    std = torch.pow(var, 0.5)\n",
    "    \n",
    "    zscores = diffs/std\n",
    "    \n",
    "    return torch.mean(torch.pow(zscores, n), dim=0)\n",
    "\n",
    "def calc_skew(prediction):\n",
    "    s = calc_moment(prediction, 3)\n",
    "    return torch.sum(torch.pow(s, 2))\n",
    "\n",
    "def calc_kurtosis(prediction):\n",
    "    k = calc_moment(prediction, 4)\n",
    "    return torch.sum(k)\n",
    "\n",
    "def calc_jarque_bera(prediction):\n",
    "    s = calc_moment(prediction, 3)\n",
    "    k = calc_moment(prediction, 4)\n",
    "    # print(f'skew = {s}, kurt = {k}')\n",
    "    \n",
    "    #n = prediction.shape[0]\n",
    "    \n",
    "    jb = (torch.pow(s,2) + torch.pow(k-3,2)/4.0)\n",
    "    # print(jb)\n",
    "    return jb\n",
    "\n",
    "def calc_out_of_range(prediction):\n",
    "    a1 = calc_average_below(prediction[:,0])\n",
    "    a2 = calc_average_below(prediction[:,1])\n",
    "    a3 = calc_average_above(prediction[:,0])\n",
    "    a4 = calc_average_above(prediction[:,1])\n",
    "    return (a1+a2+a3+a4)/4.0\n",
    "\n",
    "class decorrelate_loss:\n",
    "    '''Calculate the loss function using MSELoss and decorrelation loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._mse = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def __call__(self, prediction, labels):\n",
    "        'Calc the loss given both the correlation and mse'\n",
    "        mse_loss = self._mse(prediction, labels)\n",
    "        \n",
    "        background_mask = labels[:,1] == 0\n",
    "        r = calc_r(prediction[background_mask])\n",
    "        \n",
    "#         skew = calc_skew(prediction[background_mask])\n",
    "        \n",
    "#         spread_penalty = calc_out_of_range(prediction)\n",
    "#         print(spread_penalty)\n",
    "\n",
    "        jb = torch.sum(calc_jarque_bera(prediction[background_mask]))\n",
    "        \n",
    "        total = mse_loss + torch.square(r)*0.1 + jb\n",
    "        # print(total)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a simple classifier - 2 layers, with the same number of nodes as inputs. It looks like doubling the size of the inputs and outputs makes a big difference, but in general getting this to train better hasn't been successful so far. No idea why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_variables, n_variables*2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*2, n_variables*2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*2, n_variables*2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*2, n_variables),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables, 2))\n",
    "criterion = decorrelate_loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (0): 1.2969866394996644e-05\n",
      "Training loss (300): 1.2968875467777252e-05\n",
      "Training loss (600): 1.2968212366104127e-05\n",
      "Training loss (900): 1.2967832386493682e-05\n",
      "Training loss (1200): 1.2968741357326508e-05\n",
      "Training loss (1500): 1.2967833876609803e-05\n",
      "Training loss (1800): 1.2966649234294891e-05\n",
      "Training loss (2100): 1.2971913814544678e-05\n",
      "Training loss (2400): 1.2966738641262054e-05\n",
      "Training loss (2700): 1.2966692447662353e-05\n",
      "Training loss (3000): 1.2967328727245331e-05\n",
      "Training loss (3300): 1.2966832518577575e-05\n",
      "Training loss (3600): 1.2965019047260284e-05\n",
      "Training loss (3900): 1.296488642692566e-05\n",
      "Training loss (4200): 1.2965978682041169e-05\n",
      "Training loss (4500): 1.2964896857738494e-05\n",
      "Training loss (4800): 1.2964275479316711e-05\n",
      "Training loss (5100): 1.296561360359192e-05\n",
      "Training loss (5400): 1.296420246362686e-05\n",
      "Training loss (5700): 1.2970095872879029e-05\n",
      "Training loss (6000): 1.2963207066059113e-05\n",
      "Training loss (6300): 1.296299546957016e-05\n",
      "Training loss (6600): 1.2964741885662078e-05\n",
      "Training loss (6900): 1.296262890100479e-05\n",
      "Training loss (7200): 1.2962764501571655e-05\n",
      "Training loss (7500): 1.2964047491550446e-05\n",
      "Training loss (7800): 1.2961862981319428e-05\n",
      "Training loss (8100): 1.2961426377296448e-05\n",
      "Training loss (8400): 1.2961158156394958e-05\n",
      "Training loss (8700): 1.2964491546154022e-05\n",
      "Training loss (9000): 1.2960684299468994e-05\n",
      "Training loss (9300): 1.2962159514427185e-05\n",
      "Training loss (9600): 1.2961249053478241e-05\n",
      "Training loss (9900): 1.2960730493068695e-05\n",
      "Training loss (10200): 1.296025812625885e-05\n",
      "Training loss (10500): 1.2966203689575195e-05\n",
      "Training loss (10800): 1.2959693372249603e-05\n",
      "Training loss (11100): 1.297437697649002e-05\n",
      "Training loss (11400): 1.2960048019886017e-05\n",
      "Training loss (11700): 1.2958380579948426e-05\n",
      "Training loss (12000): 1.2957572937011719e-05\n",
      "Training loss (12300): 1.2958066165447235e-05\n",
      "Training loss (12600): 1.2959855794906616e-05\n",
      "Training loss (12900): 1.2957023084163666e-05\n",
      "Training loss (13200): 1.2958785891532898e-05\n",
      "Training loss (13500): 1.2958282232284545e-05\n",
      "Training loss (13800): 1.2957490980625153e-05\n",
      "Training loss (14100): 1.296272873878479e-05\n",
      "Training loss (14400): 1.2955200672149659e-05\n",
      "Training loss (14700): 1.2955832481384277e-05\n",
      "Training loss (15000): 1.2955276668071747e-05\n",
      "Training loss (15300): 1.2953363358974456e-05\n",
      "Training loss (15600): 1.2953530251979828e-05\n",
      "Training loss (15900): 1.295289546251297e-05\n",
      "Training loss (16200): 1.2952360510826111e-05\n",
      "Training loss (16500): 1.2952922284603119e-05\n",
      "Training loss (16800): 1.2954157590866089e-05\n",
      "Training loss (17100): 1.2952958047389984e-05\n",
      "Training loss (17400): 1.2955540418624878e-05\n",
      "Training loss (17700): 1.2952767312526703e-05\n",
      "Training loss (18000): 1.2950904667377472e-05\n",
      "Training loss (18300): 1.2952308356761932e-05\n",
      "Training loss (18600): 1.2960049510002136e-05\n",
      "Training loss (18900): 1.295139193534851e-05\n",
      "Training loss (19200): 1.295032948255539e-05\n",
      "Training loss (19500): 1.2950739264488221e-05\n",
      "Training loss (19800): 1.2954483926296233e-05\n",
      "Training loss (20100): 1.2948836386203767e-05\n",
      "Training loss (20400): 1.2952668964862823e-05\n",
      "Training loss (20700): 1.2947149574756623e-05\n",
      "Training loss (21000): 1.2948668003082276e-05\n",
      "Training loss (21300): 1.2949849665164948e-05\n",
      "Training loss (21600): 1.2951898574829101e-05\n",
      "Training loss (21900): 1.2949392199516296e-05\n",
      "Training loss (22200): 1.294606477022171e-05\n",
      "Training loss (22500): 1.2947453558444977e-05\n",
      "Training loss (22800): 1.2953421473503113e-05\n",
      "Training loss (23100): 1.294468343257904e-05\n",
      "Training loss (23400): 1.294461190700531e-05\n",
      "Training loss (23700): 1.2944774329662324e-05\n",
      "Training loss (24000): 1.2955188751220703e-05\n",
      "Training loss (24300): 1.2944306433200836e-05\n",
      "Training loss (24600): 1.2943872809410095e-05\n",
      "Training loss (24900): 1.2944605946540833e-05\n",
      "Training loss (25200): 1.2951061129570007e-05\n",
      "Training loss (25500): 1.294601857662201e-05\n",
      "Training loss (25800): 1.2944525480270386e-05\n",
      "Training loss (26100): 1.2943699955940246e-05\n",
      "Training loss (26400): 1.2944039702415467e-05\n",
      "Training loss (26700): 1.294315755367279e-05\n",
      "Training loss (27000): 1.294293999671936e-05\n",
      "Training loss (27300): 1.2947338819503785e-05\n",
      "Training loss (27600): 1.2943121790885926e-05\n",
      "Training loss (27900): 1.2942509353160858e-05\n",
      "Training loss (28200): 1.294230818748474e-05\n",
      "Training loss (28500): 1.2942811846733093e-05\n",
      "Training loss (28800): 1.2944167852401734e-05\n",
      "Training loss (29100): 1.2942405045032502e-05\n",
      "Training loss (29400): 1.294415444135666e-05\n",
      "Training loss (29700): 1.294233500957489e-05\n",
      "Training loss (30000): 1.2942975759506225e-05\n",
      "Training loss (30300): 1.2941993772983552e-05\n",
      "Training loss (30600): 1.294185221195221e-05\n",
      "Training loss (30900): 1.2946581840515138e-05\n",
      "Training loss (31200): 1.2942343950271607e-05\n",
      "Training loss (31500): 1.29476860165596e-05\n",
      "Training loss (31800): 1.294134259223938e-05\n",
      "Training loss (32100): 1.294204294681549e-05\n",
      "Training loss (32400): 1.2942475080490112e-05\n",
      "Training loss (32700): 1.2946777045726775e-05\n",
      "Training loss (33000): 1.2940880656242371e-05\n",
      "Training loss (33300): 1.2941356003284454e-05\n",
      "Training loss (33600): 1.2941370904445648e-05\n",
      "Training loss (33900): 1.2941406667232514e-05\n",
      "Training loss (34200): 1.2940964102745056e-05\n",
      "Training loss (34500): 1.294098049402237e-05\n",
      "Training loss (34800): 1.2943156063556671e-05\n",
      "Training loss (35100): 1.2940654158592224e-05\n",
      "Training loss (35400): 1.2941433489322663e-05\n",
      "Training loss (35700): 1.2942111492156982e-05\n",
      "Training loss (36000): 1.294451504945755e-05\n",
      "Training loss (36300): 1.2940782308578491e-05\n",
      "Training loss (36600): 1.2940052151679993e-05\n",
      "Training loss (36900): 1.2940244376659393e-05\n",
      "Training loss (37200): 1.29400372505188e-05\n",
      "Training loss (37500): 1.2940281629562378e-05\n",
      "Training loss (37800): 1.294129341840744e-05\n",
      "Training loss (38100): 1.2941418588161468e-05\n",
      "Training loss (38400): 1.2939858436584472e-05\n",
      "Training loss (38700): 1.2939998507499695e-05\n",
      "Training loss (39000): 1.2942004203796386e-05\n",
      "Training loss (39300): 1.2941978871822356e-05\n",
      "Training loss (39600): 1.2939521670341491e-05\n",
      "Training loss (39900): 1.2946754693984985e-05\n",
      "Training loss (40200): 1.2944315373897553e-05\n",
      "Training loss (40500): 1.2939409911632538e-05\n",
      "Training loss (40800): 1.2940399348735809e-05\n",
      "Training loss (41100): 1.2939277291297912e-05\n",
      "Training loss (41400): 1.2938939034938812e-05\n",
      "Training loss (41700): 1.2942221760749816e-05\n",
      "Training loss (42000): 1.2939450144767762e-05\n",
      "Training loss (42300): 1.2940192222595215e-05\n",
      "Training loss (42600): 1.293855905532837e-05\n",
      "Training loss (42900): 1.2940447032451629e-05\n",
      "Training loss (43200): 1.293831616640091e-05\n",
      "Training loss (43500): 1.2938684225082397e-05\n",
      "Training loss (43800): 1.2938052415847779e-05\n",
      "Training loss (44100): 1.2940435111522675e-05\n",
      "Training loss (44400): 1.2939733266830445e-05\n",
      "Training loss (44700): 1.2938089668750763e-05\n",
      "Training loss (45000): 1.2938033044338227e-05\n",
      "Training loss (45300): 1.2938039004802704e-05\n",
      "Training loss (45600): 1.2940376996994018e-05\n",
      "Training loss (45900): 1.2937727570533753e-05\n",
      "Training loss (46200): 1.2938570976257325e-05\n",
      "Training loss (46500): 1.293816864490509e-05\n",
      "Training loss (46800): 1.2939150631427764e-05\n",
      "Training loss (47100): 1.2937995791435241e-05\n",
      "Training loss (47400): 1.2939929962158203e-05\n",
      "Training loss (47700): 1.2944489717483521e-05\n",
      "Training loss (48000): 1.2938244640827179e-05\n",
      "Training loss (48300): 1.2937554717063904e-05\n",
      "Training loss (48600): 1.2937301397323608e-05\n",
      "Training loss (48900): 1.2937235832214356e-05\n",
      "Training loss (49200): 1.2938423454761506e-05\n",
      "Training loss (49500): 1.2937314808368682e-05\n",
      "Training loss (49800): 1.2937316298484803e-05\n",
      "Training loss (50100): 1.293909102678299e-05\n",
      "Training loss (50400): 1.2938183546066284e-05\n",
      "Training loss (50700): 1.2938696146011352e-05\n",
      "Training loss (51000): 1.2938418984413147e-05\n",
      "Training loss (51300): 1.2937164306640625e-05\n",
      "Training loss (51600): 1.2936559319496154e-05\n",
      "Training loss (51900): 1.2937557697296142e-05\n",
      "Training loss (52200): 1.293642669916153e-05\n",
      "Training loss (52500): 1.2936607003211974e-05\n",
      "Training loss (52800): 1.2939642369747161e-05\n",
      "Training loss (53100): 1.2941278517246247e-05\n",
      "Training loss (53400): 1.2937064468860626e-05\n",
      "Training loss (53700): 1.2939879298210143e-05\n",
      "Training loss (54000): 1.2936806678771973e-05\n",
      "Training loss (54300): 1.2944482266902923e-05\n",
      "Training loss (54600): 1.2938286364078521e-05\n",
      "Training loss (54900): 1.2939284741878509e-05\n",
      "Training loss (55200): 1.2935741245746612e-05\n",
      "Training loss (55500): 1.2937162816524506e-05\n",
      "Training loss (55800): 1.2935756146907806e-05\n",
      "Training loss (56100): 1.2935929000377655e-05\n",
      "Training loss (56400): 1.2935630977153778e-05\n",
      "Training loss (56700): 1.293569803237915e-05\n",
      "Training loss (57000): 1.2936204671859741e-05\n",
      "Training loss (57300): 1.2935775518417358e-05\n",
      "Training loss (57600): 1.2938478589057923e-05\n",
      "Training loss (57900): 1.2940114736557006e-05\n",
      "Training loss (58200): 1.2935325503349305e-05\n",
      "Training loss (58500): 1.2936557829380035e-05\n",
      "Training loss (58800): 1.2938277423381805e-05\n",
      "Training loss (59100): 1.293584406375885e-05\n",
      "Training loss (59400): 1.2935373187065125e-05\n",
      "Training loss (59700): 1.2936629354953766e-05\n",
      "Training loss (60000): 1.2935136258602143e-05\n",
      "Training loss (60300): 1.2936298549175263e-05\n",
      "Training loss (60600): 1.2937331199645996e-05\n",
      "Training loss (60900): 1.2939552962779999e-05\n",
      "Training loss (61200): 1.2936751544475556e-05\n",
      "Training loss (61500): 1.2936285138130188e-05\n",
      "Training loss (61800): 1.2934951484203338e-05\n",
      "Training loss (62100): 1.2937916815280915e-05\n",
      "Training loss (62400): 1.2937334179878234e-05\n",
      "Training loss (62700): 1.2937569618225097e-05\n",
      "Training loss (63000): 1.2935301661491394e-05\n",
      "Training loss (63300): 1.2935034930706025e-05\n",
      "Training loss (63600): 1.2936495244503021e-05\n",
      "Training loss (63900): 1.2935295701026916e-05\n",
      "Training loss (64200): 1.2935455143451691e-05\n",
      "Training loss (64500): 1.293881982564926e-05\n",
      "Training loss (64800): 1.2935569882392884e-05\n",
      "Training loss (65100): 1.2936854362487793e-05\n",
      "Training loss (65400): 1.2934686243534088e-05\n",
      "Training loss (65700): 1.293620765209198e-05\n",
      "Training loss (66000): 1.2935671210289002e-05\n",
      "Training loss (66300): 1.2934283912181854e-05\n",
      "Training loss (66600): 1.2934479117393493e-05\n",
      "Training loss (66900): 1.2937189638614655e-05\n",
      "Training loss (67200): 1.2936525046825408e-05\n",
      "Training loss (67500): 1.2936112284660339e-05\n",
      "Training loss (67800): 1.2941016256809235e-05\n",
      "Training loss (68100): 1.2935319542884827e-05\n",
      "Training loss (68400): 1.2935148179531097e-05\n",
      "Training loss (68700): 1.2936434149742127e-05\n",
      "Training loss (69000): 1.2934514880180358e-05\n",
      "Training loss (69300): 1.2938793003559112e-05\n",
      "Training loss (69600): 1.2933918833732605e-05\n",
      "Training loss (69900): 1.2935630977153778e-05\n",
      "Training loss (70200): 1.2934191524982453e-05\n",
      "Training loss (70500): 1.29340261220932e-05\n",
      "Training loss (70800): 1.2937547266483307e-05\n",
      "Training loss (71100): 1.2934018671512604e-05\n",
      "Training loss (71400): 1.2933658063411713e-05\n",
      "Training loss (71700): 1.293497085571289e-05\n",
      "Training loss (72000): 1.2936750054359437e-05\n",
      "Training loss (72300): 1.2934871017932891e-05\n",
      "Training loss (72600): 1.2933415174484254e-05\n",
      "Training loss (72900): 1.2933558225631714e-05\n",
      "Training loss (73200): 1.2934719026088715e-05\n",
      "Training loss (73500): 1.2933290004730225e-05\n",
      "Training loss (73800): 1.2933336198329926e-05\n",
      "Training loss (74100): 1.2934325635433196e-05\n",
      "Training loss (74400): 1.2933403253555298e-05\n",
      "Training loss (74700): 1.2937864661216736e-05\n",
      "Training loss (75000): 1.2932895123958587e-05\n",
      "Training loss (75300): 1.2934310734272003e-05\n",
      "Training loss (75600): 1.2932831048965454e-05\n",
      "Training loss (75900): 1.2933379411697387e-05\n",
      "Training loss (76200): 1.293720155954361e-05\n",
      "Training loss (76500): 1.2932725250720977e-05\n",
      "Training loss (76800): 1.2936145067214966e-05\n",
      "Training loss (77100): 1.2933316826820374e-05\n",
      "Training loss (77400): 1.2936526536941529e-05\n",
      "Training loss (77700): 1.2940001487731933e-05\n",
      "Training loss (78000): 1.2935931980609893e-05\n",
      "Training loss (78300): 1.2935544550418854e-05\n",
      "Training loss (78600): 1.293606162071228e-05\n",
      "Training loss (78900): 1.2935519218444824e-05\n",
      "Training loss (79200): 1.2934119999408722e-05\n",
      "Training loss (79500): 1.293342113494873e-05\n",
      "Training loss (79800): 1.2935420870780945e-05\n",
      "Training loss (80100): 1.2937180697917937e-05\n",
      "Training loss (80400): 1.2936700880527496e-05\n",
      "Training loss (80700): 1.2933123111724853e-05\n",
      "Training loss (81000): 1.2933439016342163e-05\n",
      "Training loss (81300): 1.2936785817146301e-05\n",
      "Training loss (81600): 1.2934057414531708e-05\n",
      "Training loss (81900): 1.2932300567626952e-05\n",
      "Training loss (82200): 1.2934228777885437e-05\n",
      "Training loss (82500): 1.2932516634464263e-05\n",
      "Training loss (82800): 1.2942233681678772e-05\n",
      "Training loss (83100): 1.2936925888061524e-05\n",
      "Training loss (83400): 1.2933057546615601e-05\n",
      "Training loss (83700): 1.293220967054367e-05\n",
      "Training loss (84000): 1.2932358682155609e-05\n",
      "Training loss (84300): 1.2933772802352906e-05\n",
      "Training loss (84600): 1.2934117019176484e-05\n",
      "Training loss (84900): 1.2933629751205444e-05\n",
      "Training loss (85200): 1.2936586141586303e-05\n",
      "Training loss (85500): 1.2932030856609344e-05\n",
      "Training loss (85800): 1.2932157516479492e-05\n",
      "Training loss (86100): 1.2934225797653199e-05\n",
      "Training loss (86400): 1.2936468422412872e-05\n",
      "Training loss (86700): 1.2932088971138e-05\n",
      "Training loss (87000): 1.2932094931602478e-05\n",
      "Training loss (87300): 1.2932409346103667e-05\n",
      "Training loss (87600): 1.2934212386608124e-05\n",
      "Training loss (87900): 1.2931992113590241e-05\n",
      "Training loss (88200): 1.2933599948883057e-05\n",
      "Training loss (88500): 1.2931935489177703e-05\n",
      "Training loss (88800): 1.2936481833457947e-05\n",
      "Training loss (89100): 1.2933531403541565e-05\n",
      "Training loss (89400): 1.2934072315692902e-05\n",
      "Training loss (89700): 1.2936758995056152e-05\n",
      "Training loss (90000): 1.293206512928009e-05\n",
      "Training loss (90300): 1.2931887805461883e-05\n",
      "Training loss (90600): 1.2933073937892914e-05\n",
      "Training loss (90900): 1.2932178378105163e-05\n",
      "Training loss (91200): 1.2932473421096802e-05\n",
      "Training loss (91500): 1.2932591140270232e-05\n",
      "Training loss (91800): 1.293182224035263e-05\n",
      "Training loss (92100): 1.2933532893657684e-05\n",
      "Training loss (92400): 1.2932585179805756e-05\n",
      "Training loss (92700): 1.2931662797927857e-05\n",
      "Training loss (93000): 1.293664276599884e-05\n",
      "Training loss (93300): 1.2935103476047516e-05\n",
      "Training loss (93600): 1.2932799756526946e-05\n",
      "Training loss (93900): 1.2931704521179199e-05\n",
      "Training loss (94200): 1.2931127846240998e-05\n",
      "Training loss (94500): 1.2931130826473236e-05\n",
      "Training loss (94800): 1.293589472770691e-05\n",
      "Training loss (95100): 1.2931065261363983e-05\n",
      "Training loss (95400): 1.2935164570808411e-05\n",
      "Training loss (95700): 1.2942151725292206e-05\n",
      "Training loss (96000): 1.2930825352668763e-05\n",
      "Training loss (96300): 1.2936501204967499e-05\n",
      "Training loss (96600): 1.2930826842784882e-05\n",
      "Training loss (96900): 1.2930536270141602e-05\n",
      "Training loss (97200): 1.2931714951992036e-05\n",
      "Training loss (97500): 1.2935404479503631e-05\n",
      "Training loss (97800): 1.293509602546692e-05\n",
      "Training loss (98100): 1.2932386994361877e-05\n",
      "Training loss (98400): 1.2930813431739807e-05\n",
      "Training loss (98700): 1.2943288683891297e-05\n",
      "Training loss (99000): 1.292954683303833e-05\n",
      "Training loss (99300): 1.293160766363144e-05\n",
      "Training loss (99600): 1.2932480871677399e-05\n",
      "Training loss (99900): 1.2930268049240112e-05\n",
      "Training loss (100200): 1.2930148839950562e-05\n",
      "Training loss (100500): 1.2930421531200409e-05\n",
      "Training loss (100800): 1.293078362941742e-05\n",
      "Training loss (101100): 1.2930035591125489e-05\n",
      "Training loss (101400): 1.2938039004802704e-05\n",
      "Training loss (101700): 1.2932436168193816e-05\n",
      "Training loss (102000): 1.2929059565067291e-05\n",
      "Training loss (102300): 1.294262409210205e-05\n",
      "Training loss (102600): 1.2929722666740417e-05\n",
      "Training loss (102900): 1.2929703295230866e-05\n",
      "Training loss (103200): 1.293351948261261e-05\n",
      "Training loss (103500): 1.2928856909275055e-05\n",
      "Training loss (103800): 1.2929251790046693e-05\n",
      "Training loss (104100): 1.293708086013794e-05\n",
      "Training loss (104400): 1.2931224703788757e-05\n",
      "Training loss (104700): 1.2929104268550873e-05\n",
      "Training loss (105000): 1.293020397424698e-05\n",
      "Training loss (105300): 1.2930041551589966e-05\n",
      "Training loss (105600): 1.2930944561958312e-05\n",
      "Training loss (105900): 1.293802261352539e-05\n",
      "Training loss (106200): 1.2929664552211762e-05\n",
      "Training loss (106500): 1.2929204106330872e-05\n",
      "Training loss (106800): 1.292860060930252e-05\n",
      "Training loss (107100): 1.2929610908031464e-05\n",
      "Training loss (107400): 1.2929321825504303e-05\n",
      "Training loss (107700): 1.2928737699985503e-05\n",
      "Training loss (108000): 1.2930580973625184e-05\n",
      "Training loss (108300): 1.292840838432312e-05\n",
      "Training loss (108600): 1.2933425605297088e-05\n",
      "Training loss (108900): 1.2928812205791473e-05\n",
      "Training loss (109200): 1.2933734059333802e-05\n",
      "Training loss (109500): 1.2929426133632659e-05\n",
      "Training loss (109800): 1.2934401631355286e-05\n",
      "Training loss (110100): 1.2929566204547882e-05\n",
      "Training loss (110400): 1.2931612133979797e-05\n",
      "Training loss (110700): 1.2932656705379486e-05\n",
      "Training loss (111000): 1.2928032875061035e-05\n",
      "Training loss (111300): 1.292983591556549e-05\n",
      "Training loss (111600): 1.2928125262260438e-05\n",
      "Training loss (111900): 1.2938882410526276e-05\n",
      "Training loss (112200): 1.2928067147731781e-05\n",
      "Training loss (112500): 1.2929175794124603e-05\n",
      "Training loss (112800): 1.2928374111652375e-05\n",
      "Training loss (113100): 1.2935838103294373e-05\n",
      "Training loss (113400): 1.2927773594856263e-05\n",
      "Training loss (113700): 1.2930479645729066e-05\n",
      "Training loss (114000): 1.2927721440792084e-05\n",
      "Training loss (114300): 1.2928549945354462e-05\n",
      "Training loss (114600): 1.2928174436092377e-05\n",
      "Training loss (114900): 1.2927716970443725e-05\n",
      "Training loss (115200): 1.2932896614074708e-05\n",
      "Training loss (115500): 1.2928831577301025e-05\n",
      "Training loss (115800): 1.2928280234336854e-05\n",
      "Training loss (116100): 1.2927776575088501e-05\n",
      "Training loss (116400): 1.292986124753952e-05\n",
      "Training loss (116700): 1.2930908799171448e-05\n",
      "Training loss (117000): 1.2928146123886109e-05\n",
      "Training loss (117300): 1.2927259504795074e-05\n",
      "Training loss (117600): 1.2929776310920715e-05\n",
      "Training loss (117900): 1.2928257882595062e-05\n",
      "Training loss (118200): 1.2927579879760742e-05\n",
      "Training loss (118500): 1.293141394853592e-05\n",
      "Training loss (118800): 1.2926779687404632e-05\n",
      "Training loss (119100): 1.2931950390338897e-05\n",
      "Training loss (119400): 1.292589157819748e-05\n",
      "Training loss (119700): 1.293225884437561e-05\n",
      "Training loss (120000): 1.292596459388733e-05\n",
      "Training loss (120300): 1.2925581634044648e-05\n",
      "Training loss (120600): 1.2928904592990875e-05\n",
      "Training loss (120900): 1.2925845384597779e-05\n",
      "Training loss (121200): 1.292760819196701e-05\n",
      "Training loss (121500): 1.2935149669647218e-05\n",
      "Training loss (121800): 1.2927567958831787e-05\n",
      "Training loss (122100): 1.2927871942520141e-05\n",
      "Training loss (122400): 1.2928132712841034e-05\n",
      "Training loss (122700): 1.292860209941864e-05\n",
      "Training loss (123000): 1.2926369905471802e-05\n",
      "Training loss (123300): 1.292804479598999e-05\n",
      "Training loss (123600): 1.2925933301448822e-05\n",
      "Training loss (123900): 1.2926597893238067e-05\n",
      "Training loss (124200): 1.2930859625339508e-05\n",
      "Training loss (124500): 1.2927727401256562e-05\n",
      "Training loss (124800): 1.2926027178764344e-05\n",
      "Training loss (125100): 1.2925833463668823e-05\n",
      "Training loss (125400): 1.2925916910171508e-05\n",
      "Training loss (125700): 1.2929144501686097e-05\n",
      "Training loss (126000): 1.292746216058731e-05\n",
      "Training loss (126300): 1.2925738096237183e-05\n",
      "Training loss (126600): 1.2926124036312103e-05\n",
      "Training loss (126900): 1.2925885617733002e-05\n",
      "Training loss (127200): 1.29253551363945e-05\n",
      "Training loss (127500): 1.292823702096939e-05\n",
      "Training loss (127800): 1.29251629114151e-05\n",
      "Training loss (128100): 1.2928037345409394e-05\n",
      "Training loss (128400): 1.2929661571979522e-05\n",
      "Training loss (128700): 1.2927044928073884e-05\n",
      "Training loss (129000): 1.2925830483436585e-05\n",
      "Training loss (129300): 1.292511224746704e-05\n",
      "Training loss (129600): 1.2925110757350922e-05\n",
      "Training loss (129900): 1.2932807207107543e-05\n",
      "Training loss (130200): 1.2924931943416596e-05\n",
      "Training loss (130500): 1.2932181358337403e-05\n",
      "Training loss (130800): 1.2925074994564057e-05\n",
      "Training loss (131100): 1.2930268049240112e-05\n",
      "Training loss (131400): 1.292462795972824e-05\n",
      "Training loss (131700): 1.2923505902290344e-05\n",
      "Training loss (132000): 1.292322725057602e-05\n",
      "Training loss (132300): 1.2923827767372132e-05\n",
      "Training loss (132600): 1.2923401594161988e-05\n",
      "Training loss (132900): 1.2927231192588806e-05\n",
      "Training loss (133200): 1.2924009561538697e-05\n",
      "Training loss (133500): 1.292303204536438e-05\n",
      "Training loss (133800): 1.2923730909824372e-05\n",
      "Training loss (134100): 1.292673647403717e-05\n",
      "Training loss (134400): 1.2922795116901398e-05\n",
      "Training loss (134700): 1.2922744452953338e-05\n",
      "Training loss (135000): 1.2923260033130646e-05\n",
      "Training loss (135300): 1.2924261391162872e-05\n",
      "Training loss (135600): 1.2922976911067963e-05\n",
      "Training loss (135900): 1.2923772633075715e-05\n",
      "Training loss (136200): 1.292424350976944e-05\n",
      "Training loss (136500): 1.292232871055603e-05\n",
      "Training loss (136800): 1.2922078371047974e-05\n",
      "Training loss (137100): 1.2926825881004333e-05\n",
      "Training loss (137400): 1.2924771010875702e-05\n",
      "Training loss (137700): 1.2931415438652038e-05\n",
      "Training loss (138000): 1.292833685874939e-05\n",
      "Training loss (138300): 1.2923327088356019e-05\n",
      "Training loss (138600): 1.2924076616764069e-05\n",
      "Training loss (138900): 1.2923005223274231e-05\n",
      "Training loss (139200): 1.292458176612854e-05\n",
      "Training loss (139500): 1.2922725081443787e-05\n",
      "Training loss (139800): 1.2921570241451263e-05\n",
      "Training loss (140100): 1.2922167778015137e-05\n",
      "Training loss (140400): 1.29225954413414e-05\n",
      "Training loss (140700): 1.292162388563156e-05\n",
      "Training loss (141000): 1.2922550737857819e-05\n",
      "Training loss (141300): 1.2921671569347381e-05\n",
      "Training loss (141600): 1.2921296060085297e-05\n",
      "Training loss (141900): 1.2922748923301697e-05\n",
      "Training loss (142200): 1.2920936942100526e-05\n",
      "Training loss (142500): 1.2920397520065308e-05\n",
      "Training loss (142800): 1.2920622527599335e-05\n",
      "Training loss (143100): 1.2923923134803772e-05\n",
      "Training loss (143400): 1.2921354174613952e-05\n",
      "Training loss (143700): 1.2920309603214265e-05\n",
      "Training loss (144000): 1.2921641767024994e-05\n",
      "Training loss (144300): 1.2920774519443511e-05\n",
      "Training loss (144600): 1.292194277048111e-05\n",
      "Training loss (144900): 1.2921731173992157e-05\n",
      "Training loss (145200): 1.2922319769859313e-05\n",
      "Training loss (145500): 1.2919835746288299e-05\n",
      "Training loss (145800): 1.2922361493110657e-05\n",
      "Training loss (146100): 1.292247325181961e-05\n",
      "Training loss (146400): 1.2919771671295166e-05\n",
      "Training loss (146700): 1.2925650179386139e-05\n",
      "Training loss (147000): 1.2920625507831573e-05\n",
      "Training loss (147300): 1.2919606268405914e-05\n",
      "Training loss (147600): 1.2920694053173066e-05\n",
      "Training loss (147900): 1.2918989360332489e-05\n",
      "Training loss (148200): 1.2919571995735168e-05\n",
      "Training loss (148500): 1.2919101119041444e-05\n",
      "Training loss (148800): 1.2918724119663239e-05\n",
      "Training loss (149100): 1.2919287383556366e-05\n",
      "Training loss (149400): 1.2919683754444123e-05\n",
      "Training loss (149700): 1.2929265201091767e-05\n",
      "Training loss (150000): 1.2922467291355133e-05\n",
      "Training loss (150300): 1.2919703125953675e-05\n",
      "Training loss (150600): 1.2918360531330109e-05\n",
      "Training loss (150900): 1.2919864058494569e-05\n",
      "Training loss (151200): 1.2919571995735168e-05\n",
      "Training loss (151500): 1.2918426096439362e-05\n",
      "Training loss (151800): 1.292063146829605e-05\n",
      "Training loss (152100): 1.2918379902839661e-05\n",
      "Training loss (152400): 1.2920854985713958e-05\n",
      "Training loss (152700): 1.292199194431305e-05\n",
      "Training loss (153000): 1.2918242812156677e-05\n",
      "Training loss (153300): 1.2919500470161438e-05\n",
      "Training loss (153600): 1.2921996414661408e-05\n",
      "Training loss (153900): 1.2918616831302643e-05\n",
      "Training loss (154200): 1.2919402122497558e-05\n",
      "Training loss (154500): 1.2917530536651612e-05\n",
      "Training loss (154800): 1.2917590141296387e-05\n",
      "Training loss (155100): 1.2918032705783844e-05\n",
      "Training loss (155400): 1.2920182943344116e-05\n",
      "Training loss (155700): 1.2920437753200532e-05\n",
      "Training loss (156000): 1.2921930849552155e-05\n",
      "Training loss (156300): 1.2918218970298767e-05\n",
      "Training loss (156600): 1.2918080389499664e-05\n",
      "Training loss (156900): 1.2917691469192505e-05\n",
      "Training loss (157200): 1.2920065224170684e-05\n",
      "Training loss (157500): 1.2916789948940277e-05\n",
      "Training loss (157800): 1.291911005973816e-05\n",
      "Training loss (158100): 1.2917858362197875e-05\n",
      "Training loss (158400): 1.2916603684425353e-05\n",
      "Training loss (158700): 1.2923286855220795e-05\n",
      "Training loss (159000): 1.2916988134384155e-05\n",
      "Training loss (159300): 1.2919487059116364e-05\n",
      "Training loss (159600): 1.2918394804000855e-05\n",
      "Training loss (159900): 1.291656494140625e-05\n",
      "Training loss (160200): 1.2916368246078492e-05\n",
      "Training loss (160500): 1.2916582822799682e-05\n",
      "Training loss (160800): 1.2916526198387146e-05\n",
      "Training loss (161100): 1.2916973233222961e-05\n",
      "Training loss (161400): 1.2922397255897522e-05\n",
      "Training loss (161700): 1.291743814945221e-05\n",
      "Training loss (162000): 1.2916906177997589e-05\n",
      "Training loss (162300): 1.291806548833847e-05\n",
      "Training loss (162600): 1.291867345571518e-05\n",
      "Training loss (162900): 1.29243403673172e-05\n",
      "Training loss (163200): 1.2916624546051026e-05\n",
      "Training loss (163500): 1.2916120886802673e-05\n",
      "Training loss (163800): 1.2916114926338196e-05\n",
      "Training loss (164100): 1.2916563451290131e-05\n",
      "Training loss (164400): 1.291722059249878e-05\n",
      "Training loss (164700): 1.2917663156986236e-05\n",
      "Training loss (165000): 1.2916821241378785e-05\n",
      "Training loss (165300): 1.2916485965251923e-05\n",
      "Training loss (165600): 1.291603296995163e-05\n",
      "Training loss (165900): 1.2923306226730347e-05\n",
      "Training loss (166200): 1.2915781140327454e-05\n",
      "Training loss (166500): 1.2917929887771606e-05\n",
      "Training loss (166800): 1.2915417551994324e-05\n",
      "Training loss (167100): 1.2917296588420868e-05\n",
      "Training loss (167400): 1.2925566732883454e-05\n",
      "Training loss (167700): 1.2918554246425628e-05\n",
      "Training loss (168000): 1.2916003167629241e-05\n",
      "Training loss (168300): 1.2915754318237305e-05\n",
      "Training loss (168600): 1.2917689979076385e-05\n",
      "Training loss (168900): 1.2918941676616669e-05\n",
      "Training loss (169200): 1.291595846414566e-05\n",
      "Training loss (169500): 1.2921431660652161e-05\n",
      "Training loss (169800): 1.2917323410511017e-05\n",
      "Training loss (170100): 1.2920354306697845e-05\n",
      "Training loss (170400): 1.2918512523174285e-05\n",
      "Training loss (170700): 1.2915028631687164e-05\n",
      "Training loss (171000): 1.2916865944862365e-05\n",
      "Training loss (171300): 1.2919209897518159e-05\n",
      "Training loss (171600): 1.2916918098926544e-05\n",
      "Training loss (171900): 1.2917692959308624e-05\n",
      "Training loss (172200): 1.2914952635765076e-05\n",
      "Training loss (172500): 1.291746199131012e-05\n",
      "Training loss (172800): 1.2918992340564727e-05\n",
      "Training loss (173100): 1.2914983928203582e-05\n",
      "Training loss (173400): 1.2914796173572541e-05\n",
      "Training loss (173700): 1.2914642691612244e-05\n",
      "Training loss (174000): 1.291758120059967e-05\n",
      "Training loss (174300): 1.2919008731842041e-05\n",
      "Training loss (174600): 1.291627585887909e-05\n",
      "Training loss (174900): 1.2915556132793426e-05\n",
      "Training loss (175200): 1.2918020784854888e-05\n",
      "Training loss (175500): 1.292773336172104e-05\n",
      "Training loss (175800): 1.2916897237300873e-05\n",
      "Training loss (176100): 1.2917160987854004e-05\n",
      "Training loss (176400): 1.2915696203708649e-05\n",
      "Training loss (176700): 1.2914526462554932e-05\n",
      "Training loss (177000): 1.2920019030570985e-05\n",
      "Training loss (177300): 1.291457712650299e-05\n",
      "Training loss (177600): 1.2914605438709258e-05\n",
      "Training loss (177900): 1.291435956954956e-05\n",
      "Training loss (178200): 1.2916330993175506e-05\n",
      "Training loss (178500): 1.2914419174194336e-05\n",
      "Training loss (178800): 1.291450411081314e-05\n",
      "Training loss (179100): 1.2916380167007446e-05\n",
      "Training loss (179400): 1.2914262712001801e-05\n",
      "Training loss (179700): 1.291523277759552e-05\n",
      "Training loss (180000): 1.2920914590358734e-05\n",
      "Training loss (180300): 1.2922222912311554e-05\n",
      "Training loss (180600): 1.2916921079158782e-05\n",
      "Training loss (180900): 1.2921075522899628e-05\n",
      "Training loss (181200): 1.2914040684700013e-05\n",
      "Training loss (181500): 1.2914516031742096e-05\n",
      "Training loss (181800): 1.2919533252716065e-05\n",
      "Training loss (182100): 1.2913963198661804e-05\n",
      "Training loss (182400): 1.2939786911010743e-05\n",
      "Training loss (182700): 1.291506290435791e-05\n",
      "Training loss (183000): 1.2916326522827148e-05\n",
      "Training loss (183300): 1.2914071977138519e-05\n",
      "Training loss (183600): 1.2917141616344452e-05\n",
      "Training loss (183900): 1.2917804718017577e-05\n",
      "Training loss (184200): 1.292930692434311e-05\n",
      "Training loss (184500): 1.2913918495178223e-05\n",
      "Training loss (184800): 1.2923116981983186e-05\n",
      "Training loss (185100): 1.291983723640442e-05\n",
      "Training loss (185400): 1.2913641333580018e-05\n",
      "Training loss (185700): 1.2914186716079711e-05\n",
      "Training loss (186000): 1.291385293006897e-05\n",
      "Training loss (186300): 1.2918578088283539e-05\n",
      "Training loss (186600): 1.2913766503334045e-05\n",
      "Training loss (186900): 1.2913912534713746e-05\n",
      "Training loss (187200): 1.291423887014389e-05\n",
      "Training loss (187500): 1.2913492321968078e-05\n",
      "Training loss (187800): 1.291363686323166e-05\n",
      "Training loss (188100): 1.2915383279323579e-05\n",
      "Training loss (188400): 1.2916646897792816e-05\n",
      "Training loss (188700): 1.2913967669010163e-05\n",
      "Training loss (189000): 1.2913538515567779e-05\n",
      "Training loss (189300): 1.2918974459171296e-05\n",
      "Training loss (189600): 1.2914612889289855e-05\n",
      "Training loss (189900): 1.2914428114891052e-05\n",
      "Training loss (190200): 1.2913452088832854e-05\n",
      "Training loss (190500): 1.2915478646755219e-05\n",
      "Training loss (190800): 1.2915563583374023e-05\n",
      "Training loss (191100): 1.2913365662097931e-05\n",
      "Training loss (191400): 1.291503757238388e-05\n",
      "Training loss (191700): 1.2915755808353424e-05\n",
      "Training loss (192000): 1.2913347780704498e-05\n",
      "Training loss (192300): 1.2914085388183593e-05\n",
      "Training loss (192600): 1.2916138768196107e-05\n",
      "Training loss (192900): 1.2922805547714233e-05\n",
      "Training loss (193200): 1.2916670739650726e-05\n",
      "Training loss (193500): 1.2916719913482667e-05\n",
      "Training loss (193800): 1.2914592027664184e-05\n",
      "Training loss (194100): 1.2912848591804504e-05\n",
      "Training loss (194400): 1.2916818261146545e-05\n",
      "Training loss (194700): 1.2913160026073455e-05\n",
      "Training loss (195000): 1.2912973761558533e-05\n",
      "Training loss (195300): 1.2920968234539032e-05\n",
      "Training loss (195600): 1.2914115190505982e-05\n",
      "Training loss (195900): 1.2925045192241668e-05\n",
      "Training loss (196200): 1.2912389636039734e-05\n",
      "Training loss (196500): 1.2911754846572876e-05\n",
      "Training loss (196800): 1.2911394238471986e-05\n",
      "Training loss (197100): 1.2911243736743927e-05\n",
      "Training loss (197400): 1.2922963500022888e-05\n",
      "Training loss (197700): 1.2910978496074677e-05\n",
      "Training loss (198000): 1.2920349836349488e-05\n",
      "Training loss (198300): 1.2910561263561249e-05\n",
      "Training loss (198600): 1.2912535667419434e-05\n",
      "Training loss (198900): 1.2912572920322418e-05\n",
      "Training loss (199200): 1.2919861078262329e-05\n",
      "Training loss (199500): 1.2911590933799744e-05\n",
      "Training loss (199800): 1.2910787761211396e-05\n",
      "Training loss (200100): 1.2911182641983032e-05\n",
      "Training loss (200400): 1.2912844121456146e-05\n",
      "Training loss (200700): 1.2910979986190796e-05\n",
      "Training loss (201000): 1.2912462651729584e-05\n",
      "Training loss (201300): 1.2912902235984802e-05\n",
      "Training loss (201600): 1.2911929190158844e-05\n",
      "Training loss (201900): 1.2915740907192231e-05\n",
      "Training loss (202200): 1.2910854816436768e-05\n",
      "Training loss (202500): 1.2911009788513184e-05\n",
      "Training loss (202800): 1.2909495830535888e-05\n",
      "Training loss (203100): 1.2911173701286316e-05\n",
      "Training loss (203400): 1.2909816205501557e-05\n",
      "Training loss (203700): 1.2913170456886292e-05\n",
      "Training loss (204000): 1.2910355627536774e-05\n",
      "Training loss (204300): 1.2909841537475585e-05\n",
      "Training loss (204600): 1.2911807000637055e-05\n",
      "Training loss (204900): 1.291033923625946e-05\n",
      "Training loss (205200): 1.2911050021648408e-05\n",
      "Training loss (205500): 1.2913568317890168e-05\n",
      "Training loss (205800): 1.2912824749946595e-05\n",
      "Training loss (206100): 1.2910142540931702e-05\n",
      "Training loss (206400): 1.2910895049571992e-05\n",
      "Training loss (206700): 1.2909111380577087e-05\n",
      "Training loss (207000): 1.2909750640392303e-05\n",
      "Training loss (207300): 1.2910166382789612e-05\n",
      "Training loss (207600): 1.2912769615650176e-05\n",
      "Training loss (207900): 1.291033923625946e-05\n",
      "Training loss (208200): 1.2913645803928375e-05\n",
      "Training loss (208500): 1.2910288572311402e-05\n",
      "Training loss (208800): 1.2912382185459137e-05\n",
      "Training loss (209100): 1.2910054624080658e-05\n",
      "Training loss (209400): 1.2911562621593476e-05\n",
      "Training loss (209700): 1.290813684463501e-05\n",
      "Training loss (210000): 1.2911613285541534e-05\n",
      "Training loss (210300): 1.2908762693405152e-05\n",
      "Training loss (210600): 1.2909412384033203e-05\n",
      "Training loss (210900): 1.2911976873874665e-05\n",
      "Training loss (211200): 1.2910200655460358e-05\n",
      "Training loss (211500): 1.2909239530563354e-05\n",
      "Training loss (211800): 1.2910598516464234e-05\n",
      "Training loss (212100): 1.2910528481006622e-05\n",
      "Training loss (212400): 1.2909074127674103e-05\n",
      "Training loss (212700): 1.2908941507339477e-05\n",
      "Training loss (213000): 1.2907704710960388e-05\n",
      "Training loss (213300): 1.2909731268882751e-05\n",
      "Training loss (213600): 1.2907642126083375e-05\n",
      "Training loss (213900): 1.2908613681793214e-05\n",
      "Training loss (214200): 1.290985345840454e-05\n",
      "Training loss (214500): 1.2912529706954956e-05\n",
      "Training loss (214800): 1.2911541759967803e-05\n",
      "Training loss (215100): 1.290726512670517e-05\n",
      "Training loss (215400): 1.2914709746837616e-05\n",
      "Training loss (215700): 1.2907333672046662e-05\n",
      "Training loss (216000): 1.2911480665206909e-05\n",
      "Training loss (216300): 1.2907190620899201e-05\n",
      "Training loss (216600): 1.2912069261074067e-05\n",
      "Training loss (216900): 1.290716975927353e-05\n",
      "Training loss (217200): 1.290711760520935e-05\n",
      "Training loss (217500): 1.2908531725406646e-05\n",
      "Training loss (217800): 1.2907987833023072e-05\n",
      "Training loss (218100): 1.29095658659935e-05\n",
      "Training loss (218400): 1.290707141160965e-05\n",
      "Training loss (218700): 1.290726661682129e-05\n",
      "Training loss (219000): 1.291813850402832e-05\n",
      "Training loss (219300): 1.290932446718216e-05\n",
      "Training loss (219600): 1.2907199561595917e-05\n",
      "Training loss (219900): 1.2908674776554108e-05\n",
      "Training loss (220200): 1.2908406555652619e-05\n",
      "Training loss (220500): 1.2907411158084869e-05\n",
      "Training loss (220800): 1.2909789383411408e-05\n",
      "Training loss (221100): 1.29068061709404e-05\n",
      "Training loss (221400): 1.2912830710411071e-05\n",
      "Training loss (221700): 1.2912915647029877e-05\n",
      "Training loss (222000): 1.2907518446445465e-05\n",
      "Training loss (222300): 1.2907551229000091e-05\n",
      "Training loss (222600): 1.290932148694992e-05\n",
      "Training loss (222900): 1.2906816601753234e-05\n",
      "Training loss (223200): 1.2923498451709747e-05\n",
      "Training loss (223500): 1.2906511127948761e-05\n",
      "Training loss (223800): 1.2906433641910554e-05\n",
      "Training loss (224100): 1.2916079163551331e-05\n",
      "Training loss (224400): 1.2906436622142792e-05\n",
      "Training loss (224700): 1.2907050549983978e-05\n",
      "Training loss (225000): 1.2906217575073241e-05\n",
      "Training loss (225300): 1.2906756997108459e-05\n",
      "Training loss (225600): 1.2925548851490021e-05\n",
      "Training loss (225900): 1.290641725063324e-05\n",
      "Training loss (226200): 1.2906204164028169e-05\n",
      "Training loss (226500): 1.291247010231018e-05\n",
      "Training loss (226800): 1.2910324335098267e-05\n",
      "Training loss (227100): 1.2906013429164886e-05\n",
      "Training loss (227400): 1.2908239662647247e-05\n",
      "Training loss (227700): 1.290619969367981e-05\n",
      "Training loss (228000): 1.2908796966075898e-05\n",
      "Training loss (228300): 1.2907418608665466e-05\n",
      "Training loss (228600): 1.2906517088413239e-05\n",
      "Training loss (228900): 1.2905777990818023e-05\n",
      "Training loss (229200): 1.2922912836074829e-05\n",
      "Training loss (229500): 1.2907327711582184e-05\n",
      "Training loss (229800): 1.2906625866889954e-05\n",
      "Training loss (230100): 1.2907125055789948e-05\n",
      "Training loss (230400): 1.2908516824245453e-05\n",
      "Training loss (230700): 1.2908099591732025e-05\n",
      "Training loss (231000): 1.2905611097812653e-05\n",
      "Training loss (231300): 1.2906891107559204e-05\n",
      "Training loss (231600): 1.2910363078117371e-05\n",
      "Training loss (231900): 1.2906791269779206e-05\n",
      "Training loss (232200): 1.290668249130249e-05\n",
      "Training loss (232500): 1.2909351289272309e-05\n",
      "Training loss (232800): 1.2908346951007843e-05\n",
      "Training loss (233100): 1.2909258902072906e-05\n",
      "Training loss (233400): 1.2908077239990234e-05\n",
      "Training loss (233700): 1.2906756997108459e-05\n",
      "Training loss (234000): 1.2907232344150543e-05\n",
      "Training loss (234300): 1.2905898690223693e-05\n",
      "Training loss (234600): 1.2906961143016816e-05\n",
      "Training loss (234900): 1.2906862795352936e-05\n",
      "Training loss (235200): 1.2906457483768463e-05\n",
      "Training loss (235500): 1.2910255789756775e-05\n",
      "Training loss (235800): 1.2905353307723998e-05\n",
      "Training loss (236100): 1.2906286120414734e-05\n",
      "Training loss (236400): 1.2906789779663085e-05\n",
      "Training loss (236700): 1.290535181760788e-05\n",
      "Training loss (237000): 1.2908989191055298e-05\n",
      "Training loss (237300): 1.2910239398479461e-05\n",
      "Training loss (237600): 1.290794312953949e-05\n",
      "Training loss (237900): 1.2906470894813538e-05\n",
      "Training loss (238200): 1.290903091430664e-05\n",
      "Training loss (238500): 1.2917898595333099e-05\n",
      "Training loss (238800): 1.2906262278556824e-05\n",
      "Training loss (239100): 1.2913265824317932e-05\n",
      "Training loss (239400): 1.2909136712551117e-05\n",
      "Training loss (239700): 1.2909166514873504e-05\n",
      "Training loss (240000): 1.2905256450176239e-05\n",
      "Training loss (240300): 1.2906371057033538e-05\n",
      "Training loss (240600): 1.2907865643501282e-05\n",
      "Training loss (240900): 1.2907198071479798e-05\n",
      "Training loss (241200): 1.2921284139156342e-05\n",
      "Training loss (241500): 1.2906500697135925e-05\n",
      "Training loss (241800): 1.2904991209506989e-05\n",
      "Training loss (242100): 1.2913143634796143e-05\n",
      "Training loss (242400): 1.2907980382442475e-05\n",
      "Training loss (242700): 1.2904720008373261e-05\n",
      "Training loss (243000): 1.290479153394699e-05\n",
      "Training loss (243300): 1.2904928624629975e-05\n",
      "Training loss (243600): 1.2905579805374146e-05\n",
      "Training loss (243900): 1.2906695902347564e-05\n",
      "Training loss (244200): 1.290695071220398e-05\n",
      "Training loss (244500): 1.2911762297153473e-05\n",
      "Training loss (244800): 1.2911760807037354e-05\n",
      "Training loss (245100): 1.29047229886055e-05\n",
      "Training loss (245400): 1.2922835350036621e-05\n",
      "Training loss (245700): 1.2907354533672333e-05\n",
      "Training loss (246000): 1.2906086444854736e-05\n",
      "Training loss (246300): 1.2904572486877442e-05\n",
      "Training loss (246600): 1.2908022105693817e-05\n",
      "Training loss (246900): 1.2906481325626374e-05\n",
      "Training loss (247200): 1.2906697392463685e-05\n",
      "Training loss (247500): 1.291780024766922e-05\n",
      "Training loss (247800): 1.2907542288303376e-05\n",
      "Training loss (248100): 1.2905079126358033e-05\n",
      "Training loss (248400): 1.2905821204185486e-05\n",
      "Training loss (248700): 1.2910711765289306e-05\n",
      "Training loss (249000): 1.2904728949069977e-05\n",
      "Training loss (249300): 1.2904688715934753e-05\n",
      "Training loss (249600): 1.2904424965381623e-05\n",
      "Training loss (249900): 1.2905530631542206e-05\n",
      "Training loss (250200): 1.2912824749946595e-05\n",
      "Training loss (250500): 1.290397197008133e-05\n",
      "Training loss (250800): 1.290431022644043e-05\n",
      "Training loss (251100): 1.2907184660434723e-05\n",
      "Training loss (251400): 1.2907107174396514e-05\n",
      "Training loss (251700): 1.2911640107631683e-05\n",
      "Training loss (252000): 1.2904326617717742e-05\n",
      "Training loss (252300): 1.2907566130161285e-05\n",
      "Training loss (252600): 1.2906484305858612e-05\n",
      "Training loss (252900): 1.2904761731624604e-05\n",
      "Training loss (253200): 1.2902915477752685e-05\n",
      "Training loss (253500): 1.2909108400344849e-05\n",
      "Training loss (253800): 1.290193498134613e-05\n",
      "Training loss (254100): 1.2901343405246734e-05\n",
      "Training loss (254400): 1.2906605005264282e-05\n",
      "Training loss (254700): 1.2903638184070587e-05\n",
      "Training loss (255000): 1.2902489304542541e-05\n",
      "Training loss (255300): 1.2901142239570617e-05\n",
      "Training loss (255600): 1.2908895313739776e-05\n",
      "Training loss (255900): 1.2901099026203155e-05\n",
      "Training loss (256200): 1.2907777726650239e-05\n",
      "Training loss (256500): 1.2900908291339875e-05\n",
      "Training loss (256800): 1.2909279763698577e-05\n",
      "Training loss (257100): 1.2902900576591492e-05\n",
      "Training loss (257400): 1.2910740077495576e-05\n",
      "Training loss (257700): 1.2900803983211517e-05\n",
      "Training loss (258000): 1.2905839085578918e-05\n",
      "Training loss (258300): 1.2900759279727936e-05\n",
      "Training loss (258600): 1.290518045425415e-05\n",
      "Training loss (258900): 1.2900835275650024e-05\n",
      "Training loss (259200): 1.290356069803238e-05\n",
      "Training loss (259500): 1.2901464104652405e-05\n",
      "Training loss (259800): 1.2901417911052703e-05\n",
      "Training loss (260100): 1.2900690734386445e-05\n",
      "Training loss (260400): 1.2900426983833312e-05\n",
      "Training loss (260700): 1.290123462677002e-05\n",
      "Training loss (261000): 1.2905754148960114e-05\n",
      "Training loss (261300): 1.2901903688907623e-05\n",
      "Training loss (261600): 1.2904895842075349e-05\n",
      "Training loss (261900): 1.2900431454181671e-05\n",
      "Training loss (262200): 1.2900368869304657e-05\n",
      "Training loss (262500): 1.2900348007678986e-05\n",
      "Training loss (262800): 1.2900769710540771e-05\n",
      "Training loss (263100): 1.2906308472156525e-05\n",
      "Training loss (263400): 1.2904459238052368e-05\n",
      "Training loss (263700): 1.2901821732521057e-05\n",
      "Training loss (264000): 1.290052831172943e-05\n",
      "Training loss (264300): 1.290125548839569e-05\n",
      "Training loss (264600): 1.290503591299057e-05\n",
      "Training loss (264900): 1.2906889617443085e-05\n",
      "Training loss (265200): 1.2900207936763763e-05\n",
      "Training loss (265500): 1.2904556095600128e-05\n",
      "Training loss (265800): 1.2899917364120483e-05\n",
      "Training loss (266100): 1.2900091707706452e-05\n",
      "Training loss (266400): 1.2899878621101379e-05\n",
      "Training loss (266700): 1.2902973592281342e-05\n",
      "Training loss (267000): 1.2903881072998046e-05\n",
      "Training loss (267300): 1.2900221347808838e-05\n",
      "Training loss (267600): 1.2901653349399566e-05\n",
      "Training loss (267900): 1.2902501225471497e-05\n",
      "Training loss (268200): 1.2901242077350616e-05\n",
      "Training loss (268500): 1.2908260524272918e-05\n",
      "Training loss (268800): 1.2904155254364014e-05\n",
      "Training loss (269100): 1.2899784743785858e-05\n",
      "Training loss (269400): 1.2899932265281677e-05\n",
      "Training loss (269700): 1.2901180982589721e-05\n",
      "Training loss (270000): 1.2906414270401002e-05\n",
      "Training loss (270300): 1.2904958426952362e-05\n",
      "Training loss (270600): 1.290232241153717e-05\n",
      "Training loss (270900): 1.2901028990745545e-05\n",
      "Training loss (271200): 1.2903240323066711e-05\n",
      "Training loss (271500): 1.2899771332740783e-05\n",
      "Training loss (271800): 1.2899805605411529e-05\n",
      "Training loss (272100): 1.2899672985076905e-05\n",
      "Training loss (272400): 1.2904781103134156e-05\n",
      "Training loss (272700): 1.2903220951557159e-05\n",
      "Training loss (273000): 1.2915740907192231e-05\n",
      "Training loss (273300): 1.290159821510315e-05\n",
      "Training loss (273600): 1.2903110682964325e-05\n",
      "Training loss (273900): 1.289997696876526e-05\n",
      "Training loss (274200): 1.2904152274131774e-05\n",
      "Training loss (274500): 1.2903845310211182e-05\n",
      "Training loss (274800): 1.2899644672870637e-05\n",
      "Training loss (275100): 1.2900201976299286e-05\n",
      "Training loss (275400): 1.2903086841106415e-05\n",
      "Training loss (275700): 1.290009766817093e-05\n",
      "Training loss (276000): 1.2901599705219268e-05\n",
      "Training loss (276300): 1.290447860956192e-05\n",
      "Training loss (276600): 1.2900632619857788e-05\n",
      "Training loss (276900): 1.2899844348430633e-05\n",
      "Training loss (277200): 1.2900899350643157e-05\n",
      "Training loss (277500): 1.2903705239295959e-05\n",
      "Training loss (277800): 1.2903544306755065e-05\n",
      "Training loss (278100): 1.289951354265213e-05\n",
      "Training loss (278400): 1.2904241681098938e-05\n",
      "Training loss (278700): 1.2905049324035644e-05\n",
      "Training loss (279000): 1.2900076806545258e-05\n",
      "Training loss (279300): 1.291516125202179e-05\n",
      "Training loss (279600): 1.2899689376354217e-05\n",
      "Training loss (279900): 1.2915119528770448e-05\n",
      "Training loss (280200): 1.2899537384510041e-05\n",
      "Training loss (280500): 1.2899059057235718e-05\n",
      "Training loss (280800): 1.2899908423423768e-05\n",
      "Training loss (281100): 1.291753649711609e-05\n",
      "Training loss (281400): 1.2900690734386445e-05\n",
      "Training loss (281700): 1.2902502715587616e-05\n",
      "Training loss (282000): 1.2917700409889221e-05\n",
      "Training loss (282300): 1.290285587310791e-05\n",
      "Training loss (282600): 1.2899738550186156e-05\n",
      "Training loss (282900): 1.290239542722702e-05\n",
      "Training loss (283200): 1.2901511788368225e-05\n",
      "Training loss (283500): 1.2908132374286651e-05\n",
      "Training loss (283800): 1.2903453409671784e-05\n",
      "Training loss (284100): 1.2898935377597808e-05\n",
      "Training loss (284400): 1.2899526953697204e-05\n",
      "Training loss (284700): 1.289898157119751e-05\n",
      "Training loss (285000): 1.2898972630500794e-05\n",
      "Training loss (285300): 1.2904362380504609e-05\n",
      "Training loss (285600): 1.2902714312076568e-05\n",
      "Training loss (285900): 1.2911003828048707e-05\n",
      "Training loss (286200): 1.2898540496826172e-05\n",
      "Training loss (286500): 1.2906305491924287e-05\n",
      "Training loss (286800): 1.2900881469249726e-05\n",
      "Training loss (287100): 1.2906280159950257e-05\n",
      "Training loss (287400): 1.289975494146347e-05\n",
      "Training loss (287700): 1.2902386486530304e-05\n",
      "Training loss (288000): 1.2900923192501068e-05\n",
      "Training loss (288300): 1.2903088331222534e-05\n",
      "Training loss (288600): 1.2899349629878998e-05\n",
      "Training loss (288900): 1.2899769842624664e-05\n",
      "Training loss (289200): 1.2898686528205872e-05\n",
      "Training loss (289500): 1.2904369831085206e-05\n",
      "Training loss (289800): 1.2902975082397461e-05\n",
      "Training loss (290100): 1.2901638448238373e-05\n",
      "Training loss (290400): 1.2911531329154968e-05\n",
      "Training loss (290700): 1.289970874786377e-05\n",
      "Training loss (291000): 1.2898962199687957e-05\n",
      "Training loss (291300): 1.2899680435657501e-05\n",
      "Training loss (291600): 1.2899695336818695e-05\n",
      "Training loss (291900): 1.2898190319538116e-05\n",
      "Training loss (292200): 1.290040910243988e-05\n",
      "Training loss (292500): 1.2903335690498351e-05\n",
      "Training loss (292800): 1.2905620038509369e-05\n",
      "Training loss (293100): 1.2897619605064391e-05\n",
      "Training loss (293400): 1.2897555530071259e-05\n",
      "Training loss (293700): 1.2900424003601074e-05\n",
      "Training loss (294000): 1.2899893522262573e-05\n",
      "Training loss (294300): 1.2897169589996338e-05\n",
      "Training loss (294600): 1.2910108268260956e-05\n",
      "Training loss (294900): 1.2896965444087983e-05\n",
      "Training loss (295200): 1.2896719574928283e-05\n",
      "Training loss (295500): 1.2897045910358428e-05\n",
      "Training loss (295800): 1.2898705899715423e-05\n",
      "Training loss (296100): 1.289740800857544e-05\n",
      "Training loss (296400): 1.2898911535739899e-05\n",
      "Training loss (296700): 1.2900052964687347e-05\n",
      "Training loss (297000): 1.2902940809726715e-05\n",
      "Training loss (297300): 1.2902843952178956e-05\n",
      "Training loss (297600): 1.2896369397640228e-05\n",
      "Training loss (297900): 1.2898416817188264e-05\n",
      "Training loss (298200): 1.2897633016109466e-05\n",
      "Training loss (298500): 1.2901331484317779e-05\n",
      "Training loss (298800): 1.290038377046585e-05\n",
      "Training loss (299100): 1.2897692620754241e-05\n",
      "Training loss (299400): 1.2896907329559326e-05\n",
      "Training loss (299700): 1.2908877432346345e-05\n",
      "Training loss (300000): 1.2900924682617187e-05\n",
      "Training loss (300300): 1.2900868058204651e-05\n",
      "Training loss (300600): 1.2899832427501678e-05\n",
      "Training loss (300900): 1.2896306812763214e-05\n",
      "Training loss (301200): 1.2897218763828277e-05\n",
      "Training loss (301500): 1.2897704541683197e-05\n",
      "Training loss (301800): 1.2898162007331848e-05\n",
      "Training loss (302100): 1.2895239889621735e-05\n",
      "Training loss (302400): 1.290314495563507e-05\n",
      "Training loss (302700): 1.2895728647708892e-05\n",
      "Training loss (303000): 1.2903159856796264e-05\n",
      "Training loss (303300): 1.2897948920726776e-05\n",
      "Training loss (303600): 1.2895812094211579e-05\n",
      "Training loss (303900): 1.2896284461021424e-05\n",
      "Training loss (304200): 1.2895122170448303e-05\n",
      "Training loss (304500): 1.2902945280075074e-05\n",
      "Training loss (304800): 1.2894842028617859e-05\n",
      "Training loss (305100): 1.2896434962749481e-05\n",
      "Training loss (305400): 1.2896502017974854e-05\n",
      "Training loss (305700): 1.2894818186759948e-05\n",
      "Training loss (306000): 1.2898014485836029e-05\n",
      "Training loss (306300): 1.289568990468979e-05\n",
      "Training loss (306600): 1.289462000131607e-05\n",
      "Training loss (306900): 1.2898994982242584e-05\n",
      "Training loss (307200): 1.2911127507686615e-05\n",
      "Training loss (307500): 1.2897801399230956e-05\n",
      "Training loss (307800): 1.2902183830738068e-05\n",
      "Training loss (308100): 1.2896853685379028e-05\n",
      "Training loss (308400): 1.2894728779792785e-05\n",
      "Training loss (308700): 1.2910085916519166e-05\n",
      "Training loss (309000): 1.2899143993854522e-05\n",
      "Training loss (309300): 1.2894916534423828e-05\n",
      "Training loss (309600): 1.2907016277313233e-05\n",
      "Training loss (309900): 1.2894622981548309e-05\n",
      "Training loss (310200): 1.2894350290298462e-05\n",
      "Training loss (310500): 1.2894748151302338e-05\n",
      "Training loss (310800): 1.28965824842453e-05\n",
      "Training loss (311100): 1.2894007563591003e-05\n",
      "Training loss (311400): 1.2895548343658448e-05\n",
      "Training loss (311700): 1.2895810604095458e-05\n",
      "Training loss (312000): 1.289653182029724e-05\n",
      "Training loss (312300): 1.2893861532211304e-05\n",
      "Training loss (312600): 1.2894980609416961e-05\n",
      "Training loss (312900): 1.2898685038089752e-05\n",
      "Training loss (313200): 1.2895965576171874e-05\n",
      "Training loss (313500): 1.2896965444087983e-05\n",
      "Training loss (313800): 1.2897498905658723e-05\n",
      "Training loss (314100): 1.289699524641037e-05\n",
      "Training loss (314400): 1.2893685698509217e-05\n",
      "Training loss (314700): 1.2894470989704132e-05\n",
      "Training loss (315000): 1.2894928455352784e-05\n",
      "Training loss (315300): 1.2894122302532196e-05\n",
      "Training loss (315600): 1.2893754243850708e-05\n",
      "Training loss (315900): 1.2895575165748597e-05\n",
      "Training loss (316200): 1.2894327938556672e-05\n",
      "Training loss (316500): 1.289524883031845e-05\n",
      "Training loss (316800): 1.2894468009471892e-05\n",
      "Training loss (317100): 1.2898486852645874e-05\n",
      "Training loss (317400): 1.2895719707012177e-05\n",
      "Training loss (317700): 1.2896104156970978e-05\n",
      "Training loss (318000): 1.2895545363426208e-05\n",
      "Training loss (318300): 1.2898242473602295e-05\n",
      "Training loss (318600): 1.28970205783844e-05\n",
      "Training loss (318900): 1.2894336879253387e-05\n",
      "Training loss (319200): 1.289478987455368e-05\n",
      "Training loss (319500): 1.289350986480713e-05\n",
      "Training loss (319800): 1.289636343717575e-05\n",
      "Training loss (320100): 1.2897492945194245e-05\n",
      "Training loss (320400): 1.289517879486084e-05\n",
      "Training loss (320700): 1.2895385921001433e-05\n",
      "Training loss (321000): 1.289394348859787e-05\n",
      "Training loss (321300): 1.2896065413951873e-05\n",
      "Training loss (321600): 1.2893100082874298e-05\n",
      "Training loss (321900): 1.2896902859210968e-05\n",
      "Training loss (322200): 1.2898628413677216e-05\n",
      "Training loss (322500): 1.2894439697265625e-05\n",
      "Training loss (322800): 1.289616823196411e-05\n",
      "Training loss (323100): 1.2894324958324432e-05\n",
      "Training loss (323400): 1.2893100082874298e-05\n",
      "Training loss (323700): 1.2894827127456666e-05\n",
      "Training loss (324000): 1.2895129621028901e-05\n",
      "Training loss (324300): 1.2895379960536957e-05\n",
      "Training loss (324600): 1.2893645465373994e-05\n",
      "Training loss (324900): 1.2916420400142669e-05\n",
      "Training loss (325200): 1.2899565696716309e-05\n",
      "Training loss (325500): 1.2904174625873566e-05\n",
      "Training loss (325800): 1.2910962104797364e-05\n",
      "Training loss (326100): 1.2893307209014892e-05\n",
      "Training loss (326400): 1.2917292118072509e-05\n",
      "Training loss (326700): 1.2894842028617859e-05\n",
      "Training loss (327000): 1.289341151714325e-05\n",
      "Training loss (327300): 1.2893415987491608e-05\n",
      "Training loss (327600): 1.2893739342689515e-05\n",
      "Training loss (327900): 1.2892919778823852e-05\n",
      "Training loss (328200): 1.2893287837505341e-05\n",
      "Training loss (328500): 1.2898609042167663e-05\n",
      "Training loss (328800): 1.28948375582695e-05\n",
      "Training loss (329100): 1.289462447166443e-05\n",
      "Training loss (329400): 1.2893112003803254e-05\n",
      "Training loss (329700): 1.2895742058753967e-05\n",
      "Training loss (330000): 1.2894508242607116e-05\n",
      "Training loss (330300): 1.2915444374084473e-05\n",
      "Training loss (330600): 1.2892650067806244e-05\n",
      "Training loss (330900): 1.2894274294376374e-05\n",
      "Training loss (331200): 1.289304792881012e-05\n",
      "Training loss (331500): 1.289307326078415e-05\n",
      "Training loss (331800): 1.2893044948577882e-05\n",
      "Training loss (332100): 1.289345920085907e-05\n",
      "Training loss (332400): 1.2898103892803192e-05\n",
      "Training loss (332700): 1.2893816828727722e-05\n",
      "Training loss (333000): 1.289927363395691e-05\n",
      "Training loss (333300): 1.2892699241638183e-05\n",
      "Training loss (333600): 1.292407512664795e-05\n",
      "Training loss (333900): 1.2895794212818146e-05\n",
      "Training loss (334200): 1.2895219027996064e-05\n",
      "Training loss (334500): 1.2892945110797882e-05\n",
      "Training loss (334800): 1.2892457842826844e-05\n",
      "Training loss (335100): 1.2894198298454285e-05\n",
      "Training loss (335400): 1.2892633676528931e-05\n",
      "Training loss (335700): 1.2894140183925629e-05\n",
      "Training loss (336000): 1.2892301380634307e-05\n",
      "Training loss (336300): 1.2892837822437286e-05\n",
      "Training loss (336600): 1.289280354976654e-05\n",
      "Training loss (336900): 1.2894858419895172e-05\n",
      "Training loss (337200): 1.2894244492053985e-05\n",
      "Training loss (337500): 1.2895333766937256e-05\n",
      "Training loss (337800): 1.2892429530620574e-05\n",
      "Training loss (338100): 1.2892848253250121e-05\n",
      "Training loss (338400): 1.2893998622894287e-05\n",
      "Training loss (338700): 1.2902297079563142e-05\n",
      "Training loss (339000): 1.2907467782497406e-05\n",
      "Training loss (339300): 1.2903371453285218e-05\n",
      "Training loss (339600): 1.2892864644527435e-05\n",
      "Training loss (339900): 1.2893357872962952e-05\n",
      "Training loss (340200): 1.289224624633789e-05\n",
      "Training loss (340500): 1.2910340726375579e-05\n",
      "Training loss (340800): 1.2892843782901764e-05\n",
      "Training loss (341100): 1.2894338369369506e-05\n",
      "Training loss (341400): 1.2893275916576386e-05\n",
      "Training loss (341700): 1.289433389902115e-05\n",
      "Training loss (342000): 1.2899917364120483e-05\n",
      "Training loss (342300): 1.2891201674938201e-05\n",
      "Training loss (342600): 1.2897013127803803e-05\n",
      "Training loss (342900): 1.2892040610313416e-05\n",
      "Training loss (343200): 1.289147287607193e-05\n",
      "Training loss (343500): 1.289098858833313e-05\n",
      "Training loss (343800): 1.2891843914985657e-05\n",
      "Training loss (344100): 1.2893538177013398e-05\n",
      "Training loss (344400): 1.289135217666626e-05\n",
      "Training loss (344700): 1.289093941450119e-05\n",
      "Training loss (345000): 1.2894651293754578e-05\n",
      "Training loss (345300): 1.2892772257328033e-05\n",
      "Training loss (345600): 1.2898041307926178e-05\n",
      "Training loss (345900): 1.289118230342865e-05\n",
      "Training loss (346200): 1.2897923588752747e-05\n",
      "Training loss (346500): 1.2905363738536835e-05\n",
      "Training loss (346800): 1.289464682340622e-05\n",
      "Training loss (347100): 1.2890464067459106e-05\n",
      "Training loss (347400): 1.2904384732246399e-05\n",
      "Training loss (347700): 1.289118230342865e-05\n",
      "Training loss (348000): 1.2894695997238159e-05\n",
      "Training loss (348300): 1.2892244756221771e-05\n",
      "Training loss (348600): 1.2919731438159943e-05\n",
      "Training loss (348900): 1.289667934179306e-05\n",
      "Training loss (349200): 1.2891861796379089e-05\n",
      "Training loss (349500): 1.2890568375587464e-05\n",
      "Training loss (349800): 1.28911092877388e-05\n",
      "Training loss (350100): 1.2894479930400848e-05\n",
      "Training loss (350400): 1.2901867926120758e-05\n",
      "Training loss (350700): 1.2890960276126862e-05\n",
      "Training loss (351000): 1.289030909538269e-05\n",
      "Training loss (351300): 1.2903429567813873e-05\n",
      "Training loss (351600): 1.289014220237732e-05\n",
      "Training loss (351900): 1.2890130281448365e-05\n",
      "Training loss (352200): 1.2896321713924408e-05\n",
      "Training loss (352500): 1.2890493869781495e-05\n",
      "Training loss (352800): 1.2890706956386566e-05\n",
      "Training loss (353100): 1.2891019880771638e-05\n",
      "Training loss (353400): 1.2890169024467467e-05\n",
      "Training loss (353700): 1.290038675069809e-05\n",
      "Training loss (354000): 1.2890779972076416e-05\n",
      "Training loss (354300): 1.2890443205833435e-05\n",
      "Training loss (354600): 1.2897124886512756e-05\n",
      "Training loss (354900): 1.2894107401371002e-05\n",
      "Training loss (355200): 1.289845108985901e-05\n",
      "Training loss (355500): 1.2893640995025635e-05\n",
      "Training loss (355800): 1.2892241775989533e-05\n",
      "Training loss (356100): 1.2890508770942688e-05\n",
      "Training loss (356400): 1.2890473008155824e-05\n",
      "Training loss (356700): 1.2889972329139709e-05\n",
      "Training loss (357000): 1.2890835106372833e-05\n",
      "Training loss (357300): 1.2890198826789856e-05\n",
      "Training loss (357600): 1.2890648841857911e-05\n",
      "Training loss (357900): 1.2889628112316131e-05\n",
      "Training loss (358200): 1.2890167534351348e-05\n",
      "Training loss (358500): 1.2889525294303894e-05\n",
      "Training loss (358800): 1.2889519333839416e-05\n",
      "Training loss (359100): 1.2901853024959565e-05\n",
      "Training loss (359400): 1.2891450524330139e-05\n",
      "Training loss (359700): 1.289294958114624e-05\n",
      "Training loss (360000): 1.2899737060070037e-05\n",
      "Training loss (360300): 1.2890274822711944e-05\n",
      "Training loss (360600): 1.2893472611904144e-05\n",
      "Training loss (360900): 1.289016157388687e-05\n",
      "Training loss (361200): 1.2889266014099122e-05\n",
      "Training loss (361500): 1.289043128490448e-05\n",
      "Training loss (361800): 1.2916764616966247e-05\n",
      "Training loss (362100): 1.2890301644802093e-05\n",
      "Training loss (362400): 1.2902238965034485e-05\n",
      "Training loss (362700): 1.289808452129364e-05\n",
      "Training loss (363000): 1.2889590859413147e-05\n",
      "Training loss (363300): 1.289353221654892e-05\n",
      "Training loss (363600): 1.2895949184894562e-05\n",
      "Training loss (363900): 1.2889710068702697e-05\n",
      "Training loss (364200): 1.2889170646667481e-05\n",
      "Training loss (364500): 1.289675384759903e-05\n",
      "Training loss (364800): 1.2889930605888366e-05\n",
      "Training loss (365100): 1.2889711558818818e-05\n",
      "Training loss (365400): 1.2891007959842682e-05\n",
      "Training loss (365700): 1.2889075279235839e-05\n",
      "Training loss (366000): 1.2888912856578826e-05\n",
      "Training loss (366300): 1.2890714406967163e-05\n",
      "Training loss (366600): 1.2889419496059417e-05\n",
      "Training loss (366900): 1.2889201939105987e-05\n",
      "Training loss (367200): 1.2892317771911621e-05\n",
      "Training loss (367500): 1.2896859645843506e-05\n",
      "Training loss (367800): 1.2891392409801484e-05\n",
      "Training loss (368100): 1.2889103591442109e-05\n",
      "Training loss (368400): 1.2891551852226257e-05\n",
      "Training loss (368700): 1.288975030183792e-05\n",
      "Training loss (369000): 1.288883239030838e-05\n",
      "Training loss (369300): 1.2889951467514038e-05\n",
      "Training loss (369600): 1.2890683114528657e-05\n",
      "Training loss (369900): 1.2890923023223877e-05\n",
      "Training loss (370200): 1.2895457446575165e-05\n",
      "Training loss (370500): 1.2906336784362793e-05\n",
      "Training loss (370800): 1.2889383733272552e-05\n",
      "Training loss (371100): 1.2889106571674347e-05\n",
      "Training loss (371400): 1.2894140183925629e-05\n",
      "Training loss (371700): 1.2889117002487183e-05\n",
      "Training loss (372000): 1.2899383902549744e-05\n",
      "Training loss (372300): 1.2890130281448365e-05\n",
      "Training loss (372600): 1.2898838520050049e-05\n",
      "Training loss (372900): 1.2891502678394317e-05\n",
      "Training loss (373200): 1.2890005111694336e-05\n",
      "Training loss (373500): 1.2888479232788086e-05\n",
      "Training loss (373800): 1.288895457983017e-05\n",
      "Training loss (374100): 1.2888525426387787e-05\n",
      "Training loss (374400): 1.289134919643402e-05\n",
      "Training loss (374700): 1.2888850271701813e-05\n",
      "Training loss (375000): 1.2889078259468079e-05\n",
      "Training loss (375300): 1.2907598912715912e-05\n",
      "Training loss (375600): 1.2888281047344208e-05\n",
      "Training loss (375900): 1.2888526916503906e-05\n",
      "Training loss (376200): 1.289091855287552e-05\n",
      "Training loss (376500): 1.2891878187656403e-05\n",
      "Training loss (376800): 1.289273500442505e-05\n",
      "Training loss (377100): 1.2888196110725403e-05\n",
      "Training loss (377400): 1.2888656556606292e-05\n",
      "Training loss (377700): 1.2898141145706177e-05\n",
      "Training loss (378000): 1.2888079881668091e-05\n",
      "Training loss (378300): 1.2891313433647156e-05\n",
      "Training loss (378600): 1.2890104949474335e-05\n",
      "Training loss (378900): 1.2888601422309875e-05\n",
      "Training loss (379200): 1.2891092896461486e-05\n",
      "Training loss (379500): 1.2892442941665649e-05\n",
      "Training loss (379800): 1.2890511751174926e-05\n",
      "Training loss (380100): 1.288859248161316e-05\n",
      "Training loss (380400): 1.2887737154960632e-05\n",
      "Training loss (380700): 1.2888839840888978e-05\n",
      "Training loss (381000): 1.2888288497924805e-05\n",
      "Training loss (381300): 1.2893237173557281e-05\n",
      "Training loss (381600): 1.2889157235622407e-05\n",
      "Training loss (381900): 1.2888950109481812e-05\n",
      "Training loss (382200): 1.2890294194221497e-05\n",
      "Training loss (382500): 1.2888465821743011e-05\n",
      "Training loss (382800): 1.2888209521770477e-05\n",
      "Training loss (383100): 1.2898233532905578e-05\n",
      "Training loss (383400): 1.2889949977397919e-05\n",
      "Training loss (383700): 1.2887451052665711e-05\n",
      "Training loss (384000): 1.2888157367706298e-05\n",
      "Training loss (384300): 1.2899339199066162e-05\n",
      "Training loss (384600): 1.2887394428253173e-05\n",
      "Training loss (384900): 1.2888245284557342e-05\n",
      "Training loss (385200): 1.288699209690094e-05\n",
      "Training loss (385500): 1.288761794567108e-05\n",
      "Training loss (385800): 1.288674920797348e-05\n",
      "Training loss (386100): 1.2890896201133728e-05\n",
      "Training loss (386400): 1.2887524068355561e-05\n",
      "Training loss (386700): 1.2889499962329864e-05\n",
      "Training loss (387000): 1.2888388335704804e-05\n",
      "Training loss (387300): 1.2886950373649598e-05\n",
      "Training loss (387600): 1.2889637053012848e-05\n",
      "Training loss (387900): 1.2887239456176757e-05\n",
      "Training loss (388200): 1.2887929379940034e-05\n",
      "Training loss (388500): 1.2889756262302398e-05\n",
      "Training loss (388800): 1.2893587350845337e-05\n",
      "Training loss (389100): 1.2891018390655517e-05\n",
      "Training loss (389400): 1.2888523936271668e-05\n",
      "Training loss (389700): 1.2917430698871613e-05\n",
      "Training loss (390000): 1.2891601026058198e-05\n",
      "Training loss (390300): 1.2898102402687073e-05\n",
      "Training loss (390600): 1.288606971502304e-05\n",
      "Training loss (390900): 1.2887045741081238e-05\n",
      "Training loss (391200): 1.288646012544632e-05\n",
      "Training loss (391500): 1.2886010110378265e-05\n",
      "Training loss (391800): 1.2885890901088715e-05\n",
      "Training loss (392100): 1.2886193394660949e-05\n",
      "Training loss (392400): 1.2888287007808686e-05\n",
      "Training loss (392700): 1.2887001037597656e-05\n",
      "Training loss (393000): 1.2897676229476929e-05\n",
      "Training loss (393300): 1.290498673915863e-05\n",
      "Training loss (393600): 1.2911595404148101e-05\n",
      "Training loss (393900): 1.2885917723178864e-05\n",
      "Training loss (394200): 1.2885823845863343e-05\n",
      "Training loss (394500): 1.2888000905513763e-05\n",
      "Training loss (394800): 1.2887498736381531e-05\n",
      "Training loss (395100): 1.2903505563735963e-05\n",
      "Training loss (395400): 1.2885811924934387e-05\n",
      "Training loss (395700): 1.2886078655719756e-05\n",
      "Training loss (396000): 1.2890267372131348e-05\n",
      "Training loss (396300): 1.2886884808540345e-05\n",
      "Training loss (396600): 1.2898099422454835e-05\n",
      "Training loss (396900): 1.2892045080661773e-05\n",
      "Training loss (397200): 1.2888328731060028e-05\n",
      "Training loss (397500): 1.2888352572917938e-05\n",
      "Training loss (397800): 1.2891076505184174e-05\n",
      "Training loss (398100): 1.2898693978786469e-05\n",
      "Training loss (398400): 1.2885738909244537e-05\n",
      "Training loss (398700): 1.2885405123233796e-05\n",
      "Training loss (399000): 1.2890088558197022e-05\n",
      "Training loss (399300): 1.2885671854019165e-05\n",
      "Training loss (399600): 1.2885759770870209e-05\n",
      "Training loss (399900): 1.2887530028820037e-05\n",
      "Training loss (400200): 1.2885868549346923e-05\n",
      "Training loss (400500): 1.2885157763957977e-05\n",
      "Training loss (400800): 1.288921982049942e-05\n",
      "Training loss (401100): 1.2892259657382965e-05\n",
      "Training loss (401400): 1.2891435623168945e-05\n",
      "Training loss (401700): 1.2886282801628113e-05\n",
      "Training loss (402000): 1.2888529896736146e-05\n",
      "Training loss (402300): 1.2892119586467744e-05\n",
      "Training loss (402600): 1.2907281517982483e-05\n",
      "Training loss (402900): 1.2884937226772308e-05\n",
      "Training loss (403200): 1.291445940732956e-05\n",
      "Training loss (403500): 1.2887145578861236e-05\n",
      "Training loss (403800): 1.2882278859615325e-05\n",
      "Training loss (404100): 1.2885518372058868e-05\n",
      "Training loss (404400): 1.2881967425346374e-05\n",
      "Training loss (404700): 1.2881438434123993e-05\n",
      "Training loss (405000): 1.2884005904197692e-05\n",
      "Training loss (405300): 1.2904492020606995e-05\n",
      "Training loss (405600): 1.2880605459213256e-05\n",
      "Training loss (405900): 1.288095712661743e-05\n",
      "Training loss (406200): 1.2886431813240052e-05\n",
      "Training loss (406500): 1.2882190942764282e-05\n",
      "Training loss (406800): 1.2880824506282807e-05\n",
      "Training loss (407100): 1.288057565689087e-05\n",
      "Training loss (407400): 1.2882810831069946e-05\n",
      "Training loss (407700): 1.2882848083972931e-05\n",
      "Training loss (408000): 1.2885531783103942e-05\n",
      "Training loss (408300): 1.2882104516029359e-05\n",
      "Training loss (408600): 1.2882661819458008e-05\n",
      "Training loss (408900): 1.2882548570632934e-05\n",
      "Training loss (409200): 1.2882663309574127e-05\n",
      "Training loss (409500): 1.2888418138027191e-05\n",
      "Training loss (409800): 1.2879754602909088e-05\n",
      "Training loss (410100): 1.287975013256073e-05\n",
      "Training loss (410400): 1.2883050739765168e-05\n",
      "Training loss (410700): 1.287955641746521e-05\n",
      "Training loss (411000): 1.2882040441036224e-05\n",
      "Training loss (411300): 1.288554072380066e-05\n",
      "Training loss (411600): 1.2879252433776856e-05\n",
      "Training loss (411900): 1.2879398465156556e-05\n",
      "Training loss (412200): 1.2879490852355957e-05\n",
      "Training loss (412500): 1.2879607081413268e-05\n",
      "Training loss (412800): 1.2883399426937103e-05\n",
      "Training loss (413100): 1.2887024879455567e-05\n",
      "Training loss (413400): 1.288023293018341e-05\n",
      "Training loss (413700): 1.2879255414009094e-05\n",
      "Training loss (414000): 1.2881095707416534e-05\n",
      "Training loss (414300): 1.2883272767066956e-05\n",
      "Training loss (414600): 1.288033276796341e-05\n",
      "Training loss (414900): 1.2881010770797729e-05\n",
      "Training loss (415200): 1.2887731194496154e-05\n",
      "Training loss (415500): 1.2880837917327881e-05\n",
      "Training loss (415800): 1.2878783047199249e-05\n",
      "Training loss (416100): 1.2881326675415039e-05\n",
      "Training loss (416400): 1.2879219651222229e-05\n",
      "Training loss (416700): 1.2892702221870423e-05\n",
      "Training loss (417000): 1.2904798984527587e-05\n",
      "Training loss (417300): 1.289333403110504e-05\n",
      "Training loss (417600): 1.2884531915187835e-05\n",
      "Training loss (417900): 1.2885577976703643e-05\n",
      "Training loss (418200): 1.2884005904197692e-05\n",
      "Training loss (418500): 1.2879449129104614e-05\n",
      "Training loss (418800): 1.2888820469379426e-05\n",
      "Training loss (419100): 1.2880364060401917e-05\n",
      "Training loss (419400): 1.2883879244327544e-05\n",
      "Training loss (419700): 1.2885037064552307e-05\n",
      "Training loss (420000): 1.2883880734443665e-05\n",
      "Training loss (420300): 1.2878856062889099e-05\n",
      "Training loss (420600): 1.2881782650947571e-05\n",
      "Training loss (420900): 1.2880538403987884e-05\n",
      "Training loss (421200): 1.288469284772873e-05\n",
      "Training loss (421500): 1.2889917194843292e-05\n",
      "Training loss (421800): 1.2878644466400147e-05\n",
      "Training loss (422100): 1.2889449298381806e-05\n",
      "Training loss (422400): 1.2883877754211425e-05\n",
      "Training loss (422700): 1.2891936302185058e-05\n",
      "Training loss (423000): 1.2881433963775635e-05\n",
      "Training loss (423300): 1.2879016995429993e-05\n",
      "Training loss (423600): 1.2879891693592072e-05\n",
      "Training loss (423900): 1.2879684567451477e-05\n",
      "Training loss (424200): 1.2892769277095795e-05\n",
      "Training loss (424500): 1.2887616455554962e-05\n",
      "Training loss (424800): 1.288403868675232e-05\n",
      "Training loss (425100): 1.2880863249301911e-05\n",
      "Training loss (425400): 1.288369596004486e-05\n",
      "Training loss (425700): 1.2880481779575348e-05\n",
      "Training loss (426000): 1.288147121667862e-05\n",
      "Training loss (426300): 1.2883454561233521e-05\n",
      "Training loss (426600): 1.2879343330860137e-05\n",
      "Training loss (426900): 1.2883080542087555e-05\n",
      "Training loss (427200): 1.2881527841091156e-05\n",
      "Training loss (427500): 1.2879645824432373e-05\n",
      "Training loss (427800): 1.2882782518863678e-05\n",
      "Training loss (428100): 1.2893609702587127e-05\n",
      "Training loss (428400): 1.2878313660621643e-05\n",
      "Training loss (428700): 1.2885424494743346e-05\n",
      "Training loss (429000): 1.2894405424594879e-05\n",
      "Training loss (429300): 1.2877988815307618e-05\n",
      "Training loss (429600): 1.2880025804042815e-05\n",
      "Training loss (429900): 1.28806471824646e-05\n",
      "Training loss (430200): 1.2878231704235078e-05\n",
      "Training loss (430500): 1.2878115475177764e-05\n",
      "Training loss (430800): 1.2878827750682831e-05\n",
      "Training loss (431100): 1.2878526747226714e-05\n",
      "Training loss (431400): 1.287800818681717e-05\n",
      "Training loss (431700): 1.287815123796463e-05\n",
      "Training loss (432000): 1.2882357835769653e-05\n",
      "Training loss (432300): 1.2887924909591675e-05\n",
      "Training loss (432600): 1.289483904838562e-05\n",
      "Training loss (432900): 1.2879538536071777e-05\n",
      "Training loss (433200): 1.287834495306015e-05\n",
      "Training loss (433500): 1.2885358929634095e-05\n",
      "Training loss (433800): 1.2878425419330596e-05\n",
      "Training loss (434100): 1.2880760431289672e-05\n",
      "Training loss (434400): 1.2884040176868438e-05\n",
      "Training loss (434700): 1.2880663573741913e-05\n",
      "Training loss (435000): 1.2877635657787322e-05\n",
      "Training loss (435300): 1.2877896428108215e-05\n",
      "Training loss (435600): 1.2878601253032684e-05\n",
      "Training loss (435900): 1.2878431379795074e-05\n",
      "Training loss (436200): 1.2882088124752045e-05\n",
      "Training loss (436500): 1.2877632677555084e-05\n",
      "Training loss (436800): 1.2879322469234466e-05\n",
      "Training loss (437100): 1.2878578901290893e-05\n",
      "Training loss (437400): 1.2880423665046691e-05\n",
      "Training loss (437700): 1.2889191508293152e-05\n",
      "Training loss (438000): 1.2878288328647613e-05\n",
      "Training loss (438300): 1.2885601818561554e-05\n",
      "Training loss (438600): 1.2880773842334747e-05\n",
      "Training loss (438900): 1.2888829410076141e-05\n",
      "Training loss (439200): 1.2880919873714447e-05\n",
      "Training loss (439500): 1.2878970801830291e-05\n",
      "Training loss (439800): 1.2879718840122223e-05\n",
      "Training loss (440100): 1.2918485701084136e-05\n",
      "Training loss (440400): 1.2879817187786103e-05\n",
      "Training loss (440700): 1.2878662347793578e-05\n",
      "Training loss (441000): 1.2877477705478668e-05\n",
      "Training loss (441300): 1.2877894937992096e-05\n",
      "Training loss (441600): 1.2877675890922546e-05\n",
      "Training loss (441900): 1.2881086766719819e-05\n",
      "Training loss (442200): 1.2881404161453248e-05\n",
      "Training loss (442500): 1.2878830730915069e-05\n",
      "Training loss (442800): 1.2888906896114349e-05\n",
      "Training loss (443100): 1.2883812189102174e-05\n",
      "Training loss (443400): 1.2878577411174774e-05\n",
      "Training loss (443700): 1.2911543250083924e-05\n",
      "Training loss (444000): 1.2879635393619538e-05\n",
      "Training loss (444300): 1.287795752286911e-05\n",
      "Training loss (444600): 1.2881317734718323e-05\n",
      "Training loss (444900): 1.2879376113414764e-05\n",
      "Training loss (445200): 1.2877413630485534e-05\n",
      "Training loss (445500): 1.2877273559570313e-05\n",
      "Training loss (445800): 1.2880288064479827e-05\n",
      "Training loss (446100): 1.2878203392028808e-05\n",
      "Training loss (446400): 1.288849264383316e-05\n",
      "Training loss (446700): 1.2883871793746948e-05\n",
      "Training loss (447000): 1.2878926098346711e-05\n",
      "Training loss (447300): 1.2880837917327881e-05\n",
      "Training loss (447600): 1.2880592048168182e-05\n",
      "Training loss (447900): 1.2881730496883392e-05\n",
      "Training loss (448200): 1.2882152199745179e-05\n",
      "Training loss (448500): 1.2888108193874359e-05\n",
      "Training loss (448800): 1.2877474725246429e-05\n",
      "Training loss (449100): 1.2879893183708191e-05\n",
      "Training loss (449400): 1.2884393334388733e-05\n",
      "Training loss (449700): 1.2880940735340118e-05\n",
      "Training loss (450000): 1.2879228591918945e-05\n",
      "Training loss (450300): 1.2883314490318299e-05\n",
      "Training loss (450600): 1.2884266674518585e-05\n",
      "Training loss (450900): 1.2896029651165009e-05\n",
      "Training loss (451200): 1.2888558208942414e-05\n",
      "Training loss (451500): 1.287839412689209e-05\n",
      "Training loss (451800): 1.288265585899353e-05\n",
      "Training loss (452100): 1.2877802550792694e-05\n",
      "Training loss (452400): 1.2882059812545777e-05\n",
      "Training loss (452700): 1.2877723574638366e-05\n",
      "Training loss (453000): 1.2878713011741638e-05\n",
      "Training loss (453300): 1.2879659235477447e-05\n",
      "Training loss (453600): 1.2877477705478668e-05\n",
      "Training loss (453900): 1.2877584993839264e-05\n",
      "Training loss (454200): 1.2881004810333253e-05\n",
      "Training loss (454500): 1.2880145013332367e-05\n",
      "Training loss (454800): 1.2881077826023101e-05\n",
      "Training loss (455100): 1.287941038608551e-05\n",
      "Training loss (455400): 1.2883937358856201e-05\n",
      "Training loss (455700): 1.2881112098693847e-05\n",
      "Training loss (456000): 1.289803832769394e-05\n",
      "Training loss (456300): 1.2879166007041931e-05\n",
      "Training loss (456600): 1.2886792421340942e-05\n",
      "Training loss (456900): 1.2877312302589416e-05\n",
      "Training loss (457200): 1.2882640957832336e-05\n",
      "Training loss (457500): 1.2879762053489684e-05\n",
      "Training loss (457800): 1.2878066301345825e-05\n",
      "Training loss (458100): 1.2876997888088227e-05\n",
      "Training loss (458400): 1.2881723046302795e-05\n",
      "Training loss (458700): 1.2879692018032074e-05\n",
      "Training loss (459000): 1.2881256639957428e-05\n",
      "Training loss (459300): 1.2877678871154786e-05\n",
      "Training loss (459600): 1.2878866493701935e-05\n",
      "Training loss (459900): 1.2883028388023376e-05\n",
      "Training loss (460200): 1.2881237268447876e-05\n",
      "Training loss (460500): 1.2885352969169617e-05\n",
      "Training loss (460800): 1.2877954542636872e-05\n",
      "Training loss (461100): 1.2879358232021333e-05\n",
      "Training loss (461400): 1.2877029180526733e-05\n",
      "Training loss (461700): 1.290663629770279e-05\n",
      "Training loss (462000): 1.2886610627174377e-05\n",
      "Training loss (462300): 1.287732571363449e-05\n",
      "Training loss (462600): 1.289597749710083e-05\n",
      "Training loss (462900): 1.2879423797130584e-05\n",
      "Training loss (463200): 1.2879487872123719e-05\n",
      "Training loss (463500): 1.2879481911659241e-05\n",
      "Training loss (463800): 1.2878926098346711e-05\n",
      "Training loss (464100): 1.2902061641216279e-05\n",
      "Training loss (464400): 1.2881428003311157e-05\n",
      "Training loss (464700): 1.2878762185573577e-05\n",
      "Training loss (465000): 1.2876634299755097e-05\n",
      "Training loss (465300): 1.2880970537662505e-05\n",
      "Training loss (465600): 1.2877321243286133e-05\n",
      "Training loss (465900): 1.2883883714675903e-05\n",
      "Training loss (466200): 1.2876677513122558e-05\n",
      "Training loss (466500): 1.2887781858444214e-05\n",
      "Training loss (466800): 1.2877227365970612e-05\n",
      "Training loss (467100): 1.2877926230430602e-05\n",
      "Training loss (467400): 1.2887665629386903e-05\n",
      "Training loss (467700): 1.2895672023296356e-05\n",
      "Training loss (468000): 1.2880772352218628e-05\n",
      "Training loss (468300): 1.2879182398319245e-05\n",
      "Training loss (468600): 1.2886244058609009e-05\n",
      "Training loss (468900): 1.2887579202651978e-05\n",
      "Training loss (469200): 1.2877964973449707e-05\n",
      "Training loss (469500): 1.2878534197807311e-05\n",
      "Training loss (469800): 1.2883801758289337e-05\n",
      "Training loss (470100): 1.2885716557502747e-05\n",
      "Training loss (470400): 1.2883570790290833e-05\n",
      "Training loss (470700): 1.2878233194351197e-05\n",
      "Training loss (471000): 1.287648379802704e-05\n",
      "Training loss (471300): 1.2877301871776581e-05\n",
      "Training loss (471600): 1.2878529727458954e-05\n",
      "Training loss (471900): 1.2878565490245819e-05\n",
      "Training loss (472200): 1.2883275747299194e-05\n",
      "Training loss (472500): 1.288289874792099e-05\n",
      "Training loss (472800): 1.2884558737277984e-05\n",
      "Training loss (473100): 1.288253664970398e-05\n",
      "Training loss (473400): 1.2880054116249085e-05\n",
      "Training loss (473700): 1.2878426909446717e-05\n",
      "Training loss (474000): 1.2879788875579833e-05\n",
      "Training loss (474300): 1.2884542346000672e-05\n",
      "Training loss (474600): 1.2879803776741029e-05\n",
      "Training loss (474900): 1.2876933813095092e-05\n",
      "Training loss (475200): 1.2885712087154388e-05\n",
      "Training loss (475500): 1.2881731986999511e-05\n",
      "Training loss (475800): 1.287931352853775e-05\n",
      "Training loss (476100): 1.2876871228218079e-05\n",
      "Training loss (476400): 1.2881401181221008e-05\n",
      "Training loss (476700): 1.2891891598701478e-05\n",
      "Training loss (477000): 1.287648379802704e-05\n",
      "Training loss (477300): 1.2885230779647828e-05\n",
      "Training loss (477600): 1.2880520522594452e-05\n",
      "Training loss (477900): 1.2876692414283753e-05\n",
      "Training loss (478200): 1.2883730232715606e-05\n",
      "Training loss (478500): 1.2878406047821044e-05\n",
      "Training loss (478800): 1.2876786291599273e-05\n",
      "Training loss (479100): 1.2881827354431152e-05\n",
      "Training loss (479400): 1.2883633375167847e-05\n",
      "Training loss (479700): 1.2877154350280762e-05\n",
      "Training loss (480000): 1.2877330183982849e-05\n",
      "Training loss (480300): 1.2882344424724579e-05\n",
      "Training loss (480600): 1.287958174943924e-05\n",
      "Training loss (480900): 1.2877048552036285e-05\n",
      "Training loss (481200): 1.2876932322978973e-05\n",
      "Training loss (481500): 1.2879559397697448e-05\n",
      "Training loss (481800): 1.287926286458969e-05\n",
      "Training loss (482100): 1.2876909971237183e-05\n",
      "Training loss (482400): 1.2880411744117737e-05\n",
      "Training loss (482700): 1.2881113588809968e-05\n",
      "Training loss (483000): 1.2877874076366425e-05\n",
      "Training loss (483300): 1.2876944243907929e-05\n",
      "Training loss (483600): 1.2880535423755646e-05\n",
      "Training loss (483900): 1.2885336577892303e-05\n",
      "Training loss (484200): 1.2883004546165467e-05\n",
      "Training loss (484500): 1.2881301343441009e-05\n",
      "Training loss (484800): 1.2879207730293273e-05\n",
      "Training loss (485100): 1.2876388430595399e-05\n",
      "Training loss (485400): 1.2882409989833832e-05\n",
      "Training loss (485700): 1.2878671288490296e-05\n",
      "Training loss (486000): 1.2884263694286346e-05\n",
      "Training loss (486300): 1.288350373506546e-05\n",
      "Training loss (486600): 1.2880204617977142e-05\n",
      "Training loss (486900): 1.2877748906612396e-05\n",
      "Training loss (487200): 1.287682056427002e-05\n",
      "Training loss (487500): 1.2881617248058319e-05\n",
      "Training loss (487800): 1.2878714501857757e-05\n",
      "Training loss (488100): 1.2878221273422241e-05\n",
      "Training loss (488400): 1.2879976630210876e-05\n",
      "Training loss (488700): 1.288100779056549e-05\n",
      "Training loss (489000): 1.2878014147281646e-05\n",
      "Training loss (489300): 1.287667453289032e-05\n",
      "Training loss (489600): 1.2877047061920166e-05\n",
      "Training loss (489900): 1.2876595556735992e-05\n",
      "Training loss (490200): 1.2878499925136565e-05\n",
      "Training loss (490500): 1.2876763939857483e-05\n",
      "Training loss (490800): 1.2877842783927917e-05\n",
      "Training loss (491100): 1.2876981496810912e-05\n",
      "Training loss (491400): 1.2876743078231811e-05\n",
      "Training loss (491700): 1.288415789604187e-05\n",
      "Training loss (492000): 1.2880274653434753e-05\n",
      "Training loss (492300): 1.2877711653709412e-05\n",
      "Training loss (492600): 1.2876658141613006e-05\n",
      "Training loss (492900): 1.2892240285873412e-05\n",
      "Training loss (493200): 1.2881825864315033e-05\n",
      "Training loss (493500): 1.287650614976883e-05\n",
      "Training loss (493800): 1.287965327501297e-05\n",
      "Training loss (494100): 1.2883363664150238e-05\n",
      "Training loss (494400): 1.2878799438476563e-05\n",
      "Training loss (494700): 1.2901793420314789e-05\n",
      "Training loss (495000): 1.288180649280548e-05\n",
      "Training loss (495300): 1.2878695130348205e-05\n",
      "Training loss (495600): 1.2884090840816498e-05\n",
      "Training loss (495900): 1.287865936756134e-05\n",
      "Training loss (496200): 1.2881052494049073e-05\n",
      "Training loss (496500): 1.2876740097999573e-05\n",
      "Training loss (496800): 1.288415789604187e-05\n",
      "Training loss (497100): 1.2878099083900452e-05\n",
      "Training loss (497400): 1.2878444790840149e-05\n",
      "Training loss (497700): 1.2881374359130859e-05\n",
      "Training loss (498000): 1.2880580127239228e-05\n",
      "Training loss (498300): 1.2879785895347595e-05\n",
      "Training loss (498600): 1.2887829542160034e-05\n",
      "Training loss (498900): 1.2877283990383148e-05\n",
      "Training loss (499200): 1.2877781689167023e-05\n",
      "Training loss (499500): 1.2876561284065246e-05\n",
      "Training loss (499800): 1.2880469858646392e-05\n",
      "Training loss: 1.2889227271080017e-05\n",
      "Wall time: 2h 19min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 500000\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if e % 300 == 0:\n",
    "        print(f'Training loss ({e}): {running_loss/len(x_train)}')\n",
    "else:\n",
    "    print(f'Training loss: {running_loss/len(x_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Get the predicted probabilities out and see where they line up and how well this guy did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.Tensor(testing[testing.columns[:-1]].values)\n",
    "y_test = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_results = testing.copy()\n",
    "x_results['Prediction_1'] = y_test.detach().numpy()[:,0]\n",
    "x_results['Prediction_2'] = y_test.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Prediction_1', ylabel='Count'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x_results, x='Prediction_1', hue='isSignal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Prediction_2', ylabel='Count'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x_results, x='Prediction_2', hue='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the correlation between these on background and signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Prediction_1', ylabel='Prediction_2'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=x_results, x='Prediction_1', y='Prediction_2', hue='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than something odd going on down near zero, this looks very correlated for background (and signal). We need the background to be uncorrelated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for the background\n",
    "\n",
    "Lets see what the final number of the correlation is of the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1153, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.Tensor(testing[testing.columns[-1]].values)\n",
    "mask = label == 0.0\n",
    "calc_r(y_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test[:,0].detach().numpy(), bins=50)\n",
    "plt.show()\n",
    "_ = plt.hist(y_test[:,1].detach().numpy(), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the JB test on a known normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = torch.normal(1.0, 1.0, (10000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0741)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_kurtosis(normal)-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6245e-05)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_skew(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0014)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_jarque_bera(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
