{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a regression with outputs shaped by Guassian\n",
    "\n",
    "Use pytorch to take into account the correlation during the training, and force the output distributions to be shaped by a Gaussian.\n",
    "\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variables = 4 # Number of data and background varialbes to generate\n",
    "n_samples = 10000 # Number of samples to generate\n",
    "\n",
    "center_signal = 2.0 # Where the gaussian for signal should be\n",
    "center_background = 1.0 # Where the gaussian for background should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += ['../abcdlib']\n",
    "import data_gen\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = data_gen.generate_gaussian(n_variables, center_signal, n_samples)\n",
    "back = data_gen.generate_gaussian(n_variables, center_background, n_samples)\n",
    "\n",
    "n_training_samples = int(n_samples/2)\n",
    "\n",
    "training = data_gen.combine(sig[:n_training_samples], back[:n_training_samples])\n",
    "testing = data_gen.combine(sig[n_training_samples:], back[n_training_samples:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with two outputs and a custom loss function\n",
    "\n",
    "For a regression, we need just a single output column with the 1 or 0 as the target value. Of course, this time we need 2 columns, duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = training[training.columns[-1]].values\n",
    "labels = np.stack((label, label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(training[training.columns[:-1]].values)\n",
    "y_train = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function should be a combination of the normal accuracy loss function and also the correlation on background (only), and shape the background to look like a Guassian (also only on the background).\n",
    "\n",
    "- Use the accuracy on all the training data\n",
    "- Calculate the correlation only on items marked as being background\n",
    "- Turn the background variable distirbution into a z-scoare, and then match it to a guassian curve we have pre-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r(prediction):\n",
    "    mean = torch.mean(prediction, dim=0)\n",
    "    std_dev = torch.std(prediction, dim=0)\n",
    "    parts = (prediction - mean)\n",
    "    sum = torch.sum(parts[:,0]*parts[:,1])\n",
    "    return sum / std_dev[0] / std_dev[1] / (prediction.shape[0]-1)\n",
    "\n",
    "class decorrelate_loss:\n",
    "    '''Calculate the loss function using MSELoss and decorrelation loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._mse = nn.MSELoss(reduction='mean')\n",
    "        self._unit_gaussian, _ = torch.sort(torch.randn((n_training_samples, 2)), dim=0)\n",
    "        \n",
    "    def calc_gausian_shape_difference(self, background):\n",
    "        # Turn background into z score\n",
    "        mean = torch.mean(background, dim=0)\n",
    "        std_dev = torch.std(background, dim=0)\n",
    "        z_score, _ = torch.sort((background - mean)/std_dev, dim=0)\n",
    "        \n",
    "        return self._mse(z_score, self._unit_gaussian)\n",
    "\n",
    "    def __call__(self, prediction, labels):\n",
    "        'Calc the loss given both the correlation and mse'\n",
    "        mse_loss = self._mse(prediction, labels)\n",
    "        \n",
    "        background_mask = labels[:,1] == 0\n",
    "        signal_mask = labels[:,1] == 1\n",
    "        r = calc_r(prediction[background_mask])\n",
    "        \n",
    "        g1 = self.calc_gausian_shape_difference(prediction[background_mask])\n",
    "        g2 = self.calc_gausian_shape_difference(prediction[signal_mask])\n",
    "        \n",
    "        total = mse_loss*10 + torch.square(r) + g1 + g2\n",
    "        # print(total)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a simple classifier - 2 layers, with the same number of nodes as inputs. It looks like doubling the size of the inputs and outputs makes a big difference, but in general getting this to train better hasn't been successful so far. No idea why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_variables, n_variables*2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*2, n_variables*4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*4, n_variables*16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*16, n_variables*5),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*5, n_variables*2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables*2, n_variables),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_variables, 2))\n",
    "criterion = decorrelate_loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 500000\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if e % 300 == 0:\n",
    "        print(f'Training loss ({e}): {running_loss/len(x_train)}')\n",
    "else:\n",
    "    print(f'Training loss: {running_loss/len(x_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Get the predicted probabilities out and see where they line up and how well this guy did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.Tensor(testing[testing.columns[:-1]].values)\n",
    "y_test = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_results = testing.copy()\n",
    "x_results['Prediction_1'] = y_test.detach().numpy()[:,0]\n",
    "x_results['Prediction_2'] = y_test.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x_results, x='Prediction_1', hue='isSignal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x_results, x='Prediction_2', hue='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the correlation between these on background and signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=x_results, x='Prediction_1', y='Prediction_2', col='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than something odd going on down near zero, this looks very correlated for background (and signal). We need the background to be uncorrelated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for the background\n",
    "\n",
    "Lets see what the final number of the correlation is of the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.Tensor(testing[testing.columns[-1]].values)\n",
    "mask = label == 0.0\n",
    "calc_r(y_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
