{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a regression with outputs shaped by Guassian\n",
    "\n",
    "Use pytorch to take into account the correlation during the training, and force the output distributions to be shaped by a Gaussian.\n",
    "\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variables = 4 # Number of data and background varialbes to generate\n",
    "n_samples = 5000 # Number of samples to generate\n",
    "\n",
    "center_signal = 2.0 # Where the gaussian for signal should be\n",
    "center_background = 1.0 # Where the gaussian for background should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += ['../abcdlib']\n",
    "import data_gen\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = data_gen.generate_gaussian(n_variables, center_signal, n_samples)\n",
    "back = data_gen.generate_gaussian(n_variables, center_background, n_samples)\n",
    "\n",
    "n_training_samples = int(n_samples/2)\n",
    "\n",
    "training = data_gen.combine(sig[:n_training_samples], back[:n_training_samples])\n",
    "testing = data_gen.combine(sig[n_training_samples:], back[n_training_samples:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with two outputs and a custom loss function\n",
    "\n",
    "For a regression, we need just a single output column with the 1 or 0 as the target value. Of course, this time we need 2 columns, duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = training[training.columns[-1]].values\n",
    "labels = np.stack((label, label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(training[training.columns[:-1]].values)\n",
    "y_train = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function should be a combination of the normal accuracy loss function and also the correlation on background (only), and shape the background to look like a Guassian (also only on the background).\n",
    "\n",
    "- Use the accuracy on all the training data\n",
    "- Calculate the correlation only on items marked as being background\n",
    "- Turn the background variable distirbution into a z-scoare, and then match it to a guassian curve we have pre-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dcov2(a, b):\n",
    "    outer_a = a.reshape(-1, 1) - a\n",
    "    outer_b = b.reshape(-1, 1) - b\n",
    "    \n",
    "    single_a = torch.mean(torch.abs(outer_a))\n",
    "    single_b = torch.mean(torch.abs(outer_b))\n",
    "\n",
    "    joint = torch.mean(torch.abs(outer_a)*torch.abs(outer_b))\n",
    "\n",
    "    a_outer = abs(a[:, None] - a[None, :])\n",
    "    b_outer = abs(b[:, None] - b[None, :])\n",
    "    s = torch.einsum(\"ij,ik->\", a_outer, b_outer)\n",
    "    tripple = s / len(a)**3\n",
    "    \n",
    "    dcov2 = joint + single_a*single_b - 2*tripple\n",
    "    \n",
    "    return dcov2\n",
    "\n",
    "def calc_DisCo2(prediction):\n",
    "    a = prediction[:, 0]\n",
    "    b = prediction[:, 1]\n",
    "\n",
    "    return calc_dcov2(a,b)/torch.sqrt(calc_dcov2(a,a)*calc_dcov2(b,b))\n",
    "\n",
    "class decorrelate_loss:\n",
    "    '''Calculate the loss function using MSELoss and decorrelation loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._mse = nn.MSELoss(reduction='mean')\n",
    "        self._unit_gaussian, _ = torch.sort(torch.randn((n_training_samples, 2)), dim=0)\n",
    "        self._loss_record = []\n",
    "        \n",
    "    def __call__(self, prediction, labels):\n",
    "        'Calc the loss given both the correlation and mse'\n",
    "\n",
    "        # The loss for the two predictions not matching signal/background\n",
    "        mse_loss = self._mse(prediction, labels)\n",
    "        \n",
    "        background_mask = labels[:,1] == 0\n",
    "        # signal_mask = labels[:,1] == 1\n",
    "\n",
    "        disco2 = calc_DisCo2(prediction[background_mask])\n",
    "        \n",
    "        total = mse_loss + disco2*1000.0\n",
    "        self._loss_record.append((float(mse_loss.item()),\n",
    "                                  float(disco2.item())))\n",
    "        # print(total)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a simple classifier - 2 layers, with the same number of nodes as inputs. It looks like doubling the size of the inputs and outputs makes a big difference, but in general getting this to train better hasn't been successful so far. No idea why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_variables, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 2))\n",
    "criterion = decorrelate_loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (1): 0.03571282348632813\n",
      "  Average time for last 1: 0.7853284999728203 seconds\n",
      "Training loss (301): 9.858800172805787e-05\n",
      "  Average time for last 10: 0.5276449600001797 seconds\n",
      "Training loss (601): 7.702667713165284e-05\n",
      "  Average time for last 10: 0.5235885299916845 seconds\n",
      "Training loss (901): 6.985417604446411e-05\n",
      "  Average time for last 10: 0.5280210199998692 seconds\n",
      "Training loss (1201): 6.738530993461609e-05\n",
      "  Average time for last 10: 0.5290056000056211 seconds\n",
      "Training loss (1501): 6.520920395851135e-05\n",
      "  Average time for last 10: 0.5242279499943834 seconds\n",
      "Training loss (1801): 7.015409469604492e-05\n",
      "  Average time for last 10: 0.5249053799780086 seconds\n",
      "Training loss (2101): 6.573349237442017e-05\n",
      "  Average time for last 10: 0.5454778099956457 seconds\n",
      "Training loss (2401): 6.632934808731079e-05\n",
      "  Average time for last 10: 0.5251522599777673 seconds\n",
      "Training loss (2701): 6.726318001747132e-05\n",
      "  Average time for last 10: 0.5218952799914405 seconds\n",
      "Training loss (3001): 9.806278944015504e-05\n",
      "  Average time for last 10: 0.5325690400029999 seconds\n",
      "Training loss (3301): 7.51367449760437e-05\n",
      "  Average time for last 10: 0.5207242400094401 seconds\n",
      "Training loss (3601): 9.141982197761535e-05\n",
      "  Average time for last 10: 0.5305828000069596 seconds\n",
      "Training loss (3901): 6.162765622138977e-05\n",
      "  Average time for last 10: 0.5226611200137995 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs_to_run = 5000\n",
    "epocs = 0\n",
    "import timeit\n",
    "epoc_times = []\n",
    "for e in range(epochs_to_run):\n",
    "    s_time = timeit.default_timer()\n",
    "    epocs += 1\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoc_times.append(timeit.default_timer() - s_time)\n",
    "    running_loss = loss.item()\n",
    "    if e % 300 == 0:\n",
    "        print(f'Training loss ({epocs}): {running_loss/len(x_train)}')\n",
    "        print(f'  Average time for last {len(epoc_times[-10:])}: {sum(epoc_times[-10:])/len(epoc_times[-10:])} seconds')\n",
    "else:\n",
    "    print(f'Training loss ({epocs}): {running_loss/len(x_train)}')\n",
    "    print(f'  Average time for last {len(epoc_times[-10:])}: {sum(epoc_times[-10:])/len(epoc_times[-10:])} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_info_mse = [\n",
    "    {\n",
    "        'loss': mse,\n",
    "        'type': 'mse',\n",
    "        'epoch': i\n",
    "    }\n",
    "    for i, (mse, dico2) in enumerate(criterion._loss_record)\n",
    "]\n",
    "loss_info_disco2 = [\n",
    "    {\n",
    "        'loss': disco2,\n",
    "        'type': 'DisCo^2',\n",
    "        'epoch': i\n",
    "    }\n",
    "    for i, (mse, disco2) in enumerate(criterion._loss_record)\n",
    "]\n",
    "loss_trends = pd.DataFrame(loss_info_mse + loss_info_disco2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lineplot(x='epoch', y='loss', hue='type', data=loss_trends)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Get the predicted probabilities out and see where they line up and how well this guy did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.Tensor(testing[testing.columns[:-1]].values)\n",
    "y_test = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_results = testing.copy()\n",
    "x_results['Prediction_1'] = y_test.detach().numpy()[:,0]\n",
    "x_results['Prediction_2'] = y_test.detach().numpy()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x_results, x='Prediction_1', hue='isSignal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x_results, x='Prediction_2', hue='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the correlation between these on background and signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=x_results, x='Prediction_1', y='Prediction_2', col='isSignal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than something odd going on down near zero, this looks very correlated for background (and signal). We need the background to be uncorrelated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation for the background\n",
    "\n",
    "Lets see what the final number of the correlation is of the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r(prediction):\n",
    "    mean = torch.mean(prediction, dim=0)\n",
    "    std_dev = torch.std(prediction, dim=0)\n",
    "    parts = (prediction - mean)\n",
    "    sum = torch.sum(parts[:,0]*parts[:,1])\n",
    "    return sum / std_dev[0] / std_dev[1] / (prediction.shape[0]-1)\n",
    "\n",
    "label = torch.Tensor(testing[testing.columns[-1]].values)\n",
    "mask = label == 0.0\n",
    "calc_r(y_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
